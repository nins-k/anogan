{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_components import *\n",
    "from data_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths for unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'datasets/magnetic_tiles'\n",
    "\n",
    "free_images_dir = 'MT_Free/Imgs'\n",
    "blowhole_images_dir = 'MT_Blowhole/Imgs'\n",
    "break_images_dir = 'MT_Break/Imgs'\n",
    "crack_images_dir = 'MT_Crack/Imgs'\n",
    "\n",
    "new_dataset_dir = 'datasets/rescaled_magnetic_tiles'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get names of unprocessed images and their gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_img_names, free_img_gt_names = get_img_and_gt_names(dataset_dir, free_images_dir)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(dataset_dir, crack_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and save defect images and their gt (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 57, Created 214, Conversion Rate 3.75\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(crack_img_names, \n",
    "                                crack_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, crack_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 115, Created 429, Conversion Rate 3.73\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(blowhole_img_names, \n",
    "                                blowhole_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, blowhole_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 85, Created 283, Conversion Rate 3.33\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(break_img_names, \n",
    "                                break_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, break_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and save free images (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 952, Created 2960, Conversion Rate 3.11\n"
     ]
    }
   ],
   "source": [
    "inp, op = scale_and_random_crop(free_img_names, \n",
    "                                scale_size=256, \n",
    "                                random_crop_range=50, \n",
    "                                image_count=None, \n",
    "                                save_dir=os.path.join(new_dataset_dir, free_images_dir)\n",
    "                               )\n",
    "\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op,\n",
    "                                                                   op/inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load defect image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed defect images (256x256)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(new_dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(new_dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(new_dataset_dir, crack_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load free image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "free_img_names = glob.glob(os.path.join(new_dataset_dir, free_images_dir, \"*.jpg\"))\n",
    "\n",
    "train_img_names, test_img_names = train_test_split(free_img_names, train_size=2560)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final image count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Training Images: 2560\n",
      "Free Test Images: 400\n",
      "Blowhole Test Images: 427\n",
      "Break Test Images: 278\n",
      "Crack Test Images: 213\n"
     ]
    }
   ],
   "source": [
    "print(\"Free Training Images: {}\\nFree Test Images: {}\\nBlowhole Test Images: {}\\\n",
    "\\nBreak Test Images: {}\\nCrack Test Images: {}\".format(\n",
    "                                                                                          len(train_img_names),\n",
    "                                                                                          len(test_img_names),\n",
    "                                                                                          len(blowhole_img_names),\n",
    "                                                                                          len(break_img_names),\n",
    "                                                                                          len(crack_img_names)\n",
    "                                                                                         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_generator(training_image_names, batch_size):\n",
    "    \n",
    "    while(True):\n",
    "        training_image_names = shuffle(training_image_names)\n",
    "\n",
    "        for offset in range(0, len(training_image_names), batch_size):\n",
    "            image_set_names = training_image_names[offset:batch_size+offset]\n",
    "            training_images = load_and_normalize(image_set_names)\n",
    "\n",
    "            yield training_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_generator(testing_image_names, testing_gt_image_names, test_batch_size):\n",
    "    \n",
    "    while(True):\n",
    "        testing_image_names, testing_gt_image_names = shuffle(testing_image_names, testing_gt_image_names)\n",
    "        \n",
    "        for offset in range(0, len(testing_image_names), test_batch_size):\n",
    "            image_set_names = testing_image_names[offset:test_batch_size+offset]\n",
    "            image_gt_set_names = testing_gt_image_names[offset:test_batch_size+offset]\n",
    "            \n",
    "            testing_images = load_and_normalize(image_set_names)\n",
    "            testing_gt_images = load_and_normalize(image_gt_set_names)\n",
    "            \n",
    "            yield testing_images, testing_gt_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_dir(checkpoint_dir):\n",
    "    r = glob.glob(os.path.join(checkpoint_dir, \"logs*\"))\n",
    "    log_dir_name = os.path.join(checkpoint_dir, \"logs{}\".format(str(len(r))))\n",
    "                  \n",
    "    return log_dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainx_names, testx_names, \n",
    "          blowhole_x_names, blowhole_y_names, \n",
    "          crack_x_names, crack_y_names, \n",
    "          break_x_names, break_y_names\n",
    "         ):\n",
    "    \n",
    "    penultimate_layer_units = 1024\n",
    "    latent_dimensions = 200\n",
    "    batch_size = 16\n",
    "    # FREQ_PRINT = 80\n",
    "    learning_rate = 0.00005\n",
    "    nb_epochs = 500\n",
    "    freq_epoch_test = 5\n",
    "    num_test_images = 32\n",
    "    \n",
    "    # Image input placeholder\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # Ground truth input placeholder\n",
    "    gt = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # mean and variance of the free image scores\n",
    "    mean_inp = tf.placeholder(dtype=tf.float32)\n",
    "    var_inp = tf.placeholder(dtype=tf.float32)\n",
    "    \n",
    "    # Training mode placeholder\n",
    "    training_mode = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_real_image = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=False, training_mode=True)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        z = tf.random_normal([batch_size, latent_dimensions])\n",
    "        generated_image = get_generator_model(z, reuse=False, training_mode=True)\n",
    "        regenerated_real_image = get_generator_model(encoding_real_image, reuse=True, training_mode=False)\n",
    "    \n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake, dis_fake_penultimate_layer = get_discriminator_model(generated_image, z, reuse=False, \n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "        discriminator_real, dis_real_penultimate_layer = get_discriminator_model(x, encoding_real_image, reuse=True,\n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "    \n",
    "    # Prepare labels for the loss functions\n",
    "    with tf.variable_scope('labels'):\n",
    "        \n",
    "        # Step 1\n",
    "        # Set swapped labels\n",
    "        labels_dis_enc = tf.zeros_like(discriminator_real)\n",
    "        labels_dis_gen = tf.ones_like(discriminator_fake)\n",
    "        labels_gen = tf.zeros_like(discriminator_fake)\n",
    "        labels_enc = tf.ones_like(discriminator_real)\n",
    "        \n",
    "        # Step 2\n",
    "        # Create soft labels for the discriminator\n",
    "        random_soft = tf.random_uniform(shape=(tf.shape(labels_dis_enc)), minval=0.0, maxval=0.1)\n",
    "        soft_labels_dis_enc = tf.add(labels_dis_enc, random_soft)\n",
    "        soft_labels_dis_gen = tf.subtract(labels_dis_gen, random_soft)\n",
    "\n",
    "        # Step 3\n",
    "        # With a low chance, assign noisy (swapped) labels\n",
    "        random_flip = tf.ones_like(labels_dis_enc) * tf.random_uniform(shape=(1,), minval=0, maxval=1)\n",
    "        mask = random_flip >= 0.05\n",
    "        labels_dis_enc = tf.where(mask, soft_labels_dis_enc, soft_labels_dis_gen)\n",
    "        labels_dis_gen = tf.where(mask, soft_labels_dis_gen, soft_labels_dis_enc)\n",
    "    \n",
    "    # Loss Functions\n",
    "    with tf.variable_scope('loss_functions'):\n",
    "        loss_dis_enc = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_dis_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "        loss_dis_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(discriminator_fake),\n",
    "                                                                              logits=discriminator_fake))\n",
    "        loss_discriminator = loss_dis_gen + loss_dis_enc\n",
    "        # generator\n",
    "        loss_generator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_gen,\n",
    "                                                                                logits=discriminator_fake))\n",
    "        # encoder\n",
    "        loss_encoder = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "    with tf.name_scope('optimizers'):\n",
    "        # control op dependencies for batch norm and trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        dvars = [var for var in tvars if 'discriminator_model' in var.name]\n",
    "        gvars = [var for var in tvars if 'generator_model' in var.name]\n",
    "        evars = [var for var in tvars if 'encoder_model' in var.name]\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        update_ops_gen = [x for x in update_ops if ('generator_model' in x.name)]\n",
    "        update_ops_enc = [x for x in update_ops if ('encoder_model' in x.name)]\n",
    "        update_ops_dis = [x for x in update_ops if ('discriminator_model' in x.name)]\n",
    "\n",
    "        optimizer_dis = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='dis_optimizer')\n",
    "        optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='gen_optimizer')\n",
    "        optimizer_enc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='enc_optimizer')\n",
    "\n",
    "        with tf.control_dependencies(update_ops_gen):\n",
    "            gen_op = optimizer_gen.minimize(loss_generator, var_list=gvars)\n",
    "        with tf.control_dependencies(update_ops_enc):\n",
    "            enc_op = optimizer_enc.minimize(loss_encoder, var_list=evars, global_step=tf.train.get_or_create_global_step())\n",
    "        with tf.control_dependencies(update_ops_dis):\n",
    "            dis_op = optimizer_dis.minimize(loss_discriminator, var_list=dvars)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        with tf.name_scope('discriminator'):\n",
    "            tf.summary.scalar('loss_total', loss_discriminator, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_enc', loss_dis_enc, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_gen', loss_dis_gen, ['dis'])\n",
    "\n",
    "        with tf.name_scope('generator'):\n",
    "            tf.summary.scalar('loss_generator', loss_generator, ['gen'])\n",
    "            tf.summary.scalar('loss_encoder', loss_encoder, ['gen'])\n",
    "\n",
    "    with tf.name_scope('train_img_regen'):\n",
    "        for p in range(4):\n",
    "            tf.summary.image('img_{}_regen'.format(p+1), regenerated_real_image[p:p+1,:,:,:], 1, ['image'])\n",
    "            tf.summary.image('img_{}_input'.format(p+1), x[p:p+1,:,:,:], 1, ['image'])\n",
    "\n",
    "    sum_op_dis = tf.summary.merge_all('dis')\n",
    "    sum_op_gen = tf.summary.merge_all('gen')\n",
    "    sum_op_im = tf.summary.merge_all('image')\n",
    "\n",
    "        \n",
    "    '''\n",
    "    ----------------------------------------TRAINING OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "        \n",
    "    # TESTING GRAPH\n",
    "\n",
    "\n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_test = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=True, training_mode=False)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        regenerated_image_test = get_generator_model(encoding_test, reuse=True, training_mode=False)\n",
    "\n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake_test, dis_fake_penultimate_layer_test = get_discriminator_model(regenerated_image_test, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True, \n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "        discriminator_real_test, dis_real_penultimate_layer_test = get_discriminator_model(x, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True,\n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "    with tf.name_scope('testing'):\n",
    "        with tf.variable_scope('reconstruction_loss'):\n",
    "            delta = x - regenerated_image_test\n",
    "            delta_flat = tf.layers.flatten(delta)\n",
    "            gen_score = tf.norm(delta_flat, ord='euclidean', axis=1,\n",
    "                              keep_dims=False, name='epsilon')\n",
    "\n",
    "        with tf.variable_scope('discriminator_loss'):\n",
    "            fm = dis_real_penultimate_layer_test - dis_fake_penultimate_layer_test\n",
    "            fm = tf.contrib.layers.flatten(fm)\n",
    "            dis_score = tf.norm(fm, ord='euclidean', axis=1,\n",
    "                             keep_dims=False, name='d_loss')\n",
    "            dis_score = tf.squeeze(dis_score)\n",
    "\n",
    "            \n",
    "        weight1, weight2, weight3, weight4, weight5 = 0.1, 0.2, 0.3, 0.4, 0.5 \n",
    "        \n",
    "        with tf.variable_scope('score'):\n",
    "            mean_score1 = tf.reduce_mean((1 - weight1) * gen_score + weight1 * dis_score)\n",
    "            mean_score2 = tf.reduce_mean((1 - weight2) * gen_score + weight2 * dis_score)\n",
    "            mean_score3 = tf.reduce_mean((1 - weight3) * gen_score + weight3 * dis_score)\n",
    "            mean_score4 = tf.reduce_mean((1 - weight4) * gen_score + weight4 * dis_score)\n",
    "            mean_score5 = tf.reduce_mean((1 - weight5) * gen_score + weight5 * dis_score)\n",
    "            \n",
    "\n",
    "    with tf.name_scope('test_anomaly_score'):\n",
    "        tf.summary.scalar(\"mean_score_w=0.1\", mean_score1, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.2\", mean_score2, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.3\", mean_score3, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.4\", mean_score4, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.5\", mean_score5, ['scr'])\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('accuracy'):\n",
    "        \n",
    "        # For defect accuracy calculation\n",
    "        all_test_scores = (1 - weight1) * gen_score + weight1 * dis_score\n",
    "        free_thresh_0 = mean_inp\n",
    "        free_thresh_1 = mean_inp + tf.sqrt(var_inp)\n",
    "        free_thresh_2 = mean_inp + 2* tf.sqrt(var_inp)\n",
    "        free_thresh_3 = mean_inp + 3 * tf.sqrt(var_inp)\n",
    "        \n",
    "        bool_list_0 = tf.greater_equal(all_test_scores, free_thresh_0)\n",
    "        test_acc_0 = tf.reduce_sum(tf.cast(bool_list_0, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_1 = tf.greater_equal(all_test_scores, free_thresh_1)\n",
    "        test_acc_1 = tf.reduce_sum(tf.cast(bool_list_1, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_2 = tf.greater_equal(all_test_scores, free_thresh_2)\n",
    "        test_acc_2 = tf.reduce_sum(tf.cast(bool_list_2, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_3 = tf.greater_equal(all_test_scores, free_thresh_3)\n",
    "        test_acc_3 = tf.reduce_sum(tf.cast(bool_list_3, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        # For calculating optimal anomaly score based on free image scores\n",
    "        mean, var = tf.nn.moments(all_test_scores, axes=[0])\n",
    "\n",
    "        \n",
    "    \n",
    "    with tf.name_scope('test_accuracy'):\n",
    "        \n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_0', test_acc_0, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_1', test_acc_1, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_2', test_acc_2, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_3', test_acc_3, ['test_acc'])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('test_img_regen'):\n",
    "        for p in range(2):\n",
    "            tf.summary.image('{}_0_input'.format(p+1), x[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_1_regen'.format(p+1), regenerated_image_test[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_2_ground_truth'.format(p+1), gt[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_3_difference'.format(p+1), delta[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            \n",
    "            \n",
    "    sum_op_scr = tf.summary.merge_all('scr')\n",
    "    sum_op_t_img = tf.summary.merge_all('t_image')\n",
    "    sum_op_test_acc = tf.summary.merge_all('test_acc')\n",
    "    \n",
    "    gs = tf.train.get_global_step()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ----------------------------------------TEST OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    # TRAINING\n",
    "    \n",
    "    train_gen = training_generator(trainx_names, batch_size=batch_size)\n",
    "    test_gen = training_generator(testx_names, batch_size=batch_size)\n",
    "    \n",
    "    blowhole_gen = testing_generator(blowhole_x_names, blowhole_y_names, test_batch_size=num_test_images)\n",
    "    crack_gen = testing_generator(crack_x_names, crack_y_names, test_batch_size=num_test_images)\n",
    "    break_gen = testing_generator(break_x_names, break_y_names, test_batch_size=num_test_images)\n",
    "               \n",
    "            \n",
    "    checkpoint_dir = \"train/train01/\"\n",
    "    summary_dir = get_summary_dir(checkpoint_dir)\n",
    "    \n",
    "    free_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"free\"))\n",
    "    blowhole_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"blowhole\"))\n",
    "    crack_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"crack\"))\n",
    "    break_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"break\"))\n",
    "\n",
    "\n",
    "    step_saver =tf.train.CheckpointSaverHook(checkpoint_dir=checkpoint_dir, save_steps=1600, save_secs=None)\n",
    "\n",
    "    summary_saver = tf.train.SummarySaverHook(save_steps=8,\n",
    "                                              save_secs=None,\n",
    "                                              output_dir=summary_dir, \n",
    "                                              summary_op=[sum_op_dis, sum_op_gen, sum_op_im]\n",
    "                                             )\n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    \n",
    "    mnt = tf.train.MonitoredTrainingSession(config=config, hooks=[step_saver, summary_saver])\n",
    "\n",
    "\n",
    "    \n",
    "    with mnt as sess:\n",
    "        \n",
    "        \n",
    "        train_batch = 0\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "        while not mnt.should_stop() and epoch < nb_epochs:\n",
    "\n",
    "            begin = time.time()\n",
    "            nr_batches_train = int(len(trainx_names) / batch_size)\n",
    "\n",
    "            # get data using generator\n",
    "            trainx = next(train_gen)\n",
    "            \n",
    "            train_loss_dis, train_loss_gen, train_loss_enc = [0, 0, 0]\n",
    "\n",
    "            # training\n",
    "            for t in range(nr_batches_train):\n",
    "\n",
    "                print(\"Starting Epoch {}, Batch {}, Step {}\".format(epoch+1, t+1, step+1))     \n",
    "                ran_from = t * batch_size\n",
    "                ran_to = (t + 1) * batch_size\n",
    "\n",
    "                # train discriminator\n",
    "                feed_dict = {x: trainx,\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "\n",
    "                _, ld, sm = sess.run([dis_op,\n",
    "                                      loss_discriminator,\n",
    "                                      sum_op_dis],\n",
    "                                     feed_dict=feed_dict)\n",
    "                train_loss_dis += ld\n",
    "\n",
    "                # train generator and encoder\n",
    "                feed_dict = {x: trainx,\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "                _,_, le, lg, sm = sess.run([gen_op,\n",
    "                                            enc_op,\n",
    "                                            loss_encoder,\n",
    "                                            loss_generator,\n",
    "                                            sum_op_gen],\n",
    "                                           feed_dict=feed_dict)\n",
    "                train_loss_gen += lg\n",
    "                train_loss_enc += le\n",
    "\n",
    "\n",
    "                train_batch += 1\n",
    "                step+=1\n",
    "\n",
    "            train_loss_gen /= nr_batches_train\n",
    "            train_loss_enc /= nr_batches_train\n",
    "            train_loss_dis /= nr_batches_train\n",
    "\n",
    "            print(\"Epoch %d | time = %ds | loss gen = %.4f | loss enc = %.4f | loss dis = %.4f \"\n",
    "                  % (epoch+1, time.time() - begin, train_loss_gen, train_loss_enc, train_loss_dis))\n",
    "            \n",
    "            # Inspect reconstruction\n",
    "            if (epoch+1) % freq_epoch_test == 0:  \n",
    "                    ran_from = 0\n",
    "                    ran_to =  4\n",
    "                    sm = sess.run(sum_op_im, feed_dict={x: trainx[ran_from:ran_to],training_mode: False})\n",
    "            \n",
    "            # Test\n",
    "            \n",
    "            if (epoch+1) % freq_epoch_test == 0:\n",
    "                print(\"Evaluating\")\n",
    "                \n",
    "                # Shuffle\n",
    "                testx = next(test_gen)\n",
    "\n",
    "                blowhole_x, blowhole_y = next(blowhole_gen)\n",
    "                break_x, break_y = next(break_gen)\n",
    "                crack_x, crack_y = next(crack_gen)\n",
    "                \n",
    "\n",
    "                # Free Test\n",
    "                free_score_summary, free_t_img_summary, current_step, mean_score, var_score = \\\n",
    "                                                                            sess.run([sum_op_scr, sum_op_t_img, gs,\n",
    "                                                                                     mean, var], \n",
    "                                                                            feed_dict={x: testx,\n",
    "                                                                                       gt: np.zeros_like(testx),\n",
    "                                                                                       mean_inp: 0,\n",
    "                                                                                       var_inp: 0,\n",
    "                                                                                       training_mode: False})\n",
    "                free_writer.add_summary(free_score_summary, current_step)\n",
    "                free_writer.add_summary(free_t_img_summary, current_step)\n",
    "                free_writer.flush()\n",
    "                \n",
    "\n",
    "                # Blowhole\n",
    "                blowhole_score_summary, blowhole_t_img_summary, blowhole_acc_summary = \\\n",
    "                                                                          sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                          feed_dict={x: blowhole_x,\n",
    "                                                                                     gt: blowhole_y,\n",
    "                                                                                     mean_inp: mean_score,\n",
    "                                                                                     var_inp: var_score,\n",
    "                                                                                     training_mode: False})\n",
    "                blowhole_writer.add_summary(blowhole_score_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_t_img_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_acc_summary, current_step)\n",
    "                blowhole_writer.flush()\n",
    "                \n",
    "                # Crack\n",
    "                crack_score_summary, crack_t_img_summary, crack_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: crack_x, \n",
    "                                                                               gt: crack_y,\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                crack_writer.add_summary(crack_score_summary, current_step)\n",
    "                crack_writer.add_summary(crack_t_img_summary, current_step)\n",
    "                crack_writer.add_summary(crack_acc_summary, current_step)\n",
    "                crack_writer.flush()\n",
    "                    \n",
    "                # Break\n",
    "                break_score_summary, break_t_img_summary, break_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: break_x,\n",
    "                                                                               gt: break_y,\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                break_writer.add_summary(break_score_summary, current_step)\n",
    "                break_writer.add_summary(break_t_img_summary, current_step)\n",
    "                break_writer.add_summary(break_acc_summary, current_step)\n",
    "                break_writer.flush()\n",
    "                \n",
    "            \n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (16, 200)\n",
      "Output shape of layer_01 is (16, 8, 8, 1024)\n",
      "Output shape of layer_02 is (16, 16, 16, 512)\n",
      "Output shape of layer_03 is (16, 32, 32, 256)\n",
      "Output shape of layer_04 is (16, 64, 64, 128)\n",
      "Output shape of layer_05 is (16, 128, 128, 64)\n",
      "Output shape of layer_06 is (16, 256, 256, 3)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (16, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (16, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (16, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (16, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (16, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (16, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (16, 200)\n",
      "Output shape of z_layer_01 is (16, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (16, 66560)\n",
      "Output shape of xz_layer_01 is (16, 1024)\n",
      "Output shape of xz_layer_02 is (16, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "INFO:tensorflow:Summary name mean_score_w=0.1 is illegal; using mean_score_w_0.1 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.2 is illegal; using mean_score_w_0.2 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.3 is illegal; using mean_score_w_0.3 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.4 is illegal; using mean_score_w_0.4 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.5 is illegal; using mean_score_w_0.5 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_0 is illegal; using threshold_with_w_0.1__stddev_0 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_1 is illegal; using threshold_with_w_0.1__stddev_1 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_2 is illegal; using threshold_with_w_0.1__stddev_2 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_3 is illegal; using threshold_with_w_0.1__stddev_3 instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/train01/model.ckpt.\n",
      "Starting Epoch 1, Batch 1, Step 1\n",
      "Starting Epoch 1, Batch 2, Step 2\n",
      "Epoch 1 | time = 27s | loss gen = 0.8854 | loss enc = 0.6124 | loss dis = 1.8171 \n",
      "Starting Epoch 2, Batch 1, Step 3\n",
      "Starting Epoch 2, Batch 2, Step 4\n",
      "Epoch 2 | time = 2s | loss gen = 0.8996 | loss enc = 0.7240 | loss dis = 2.1435 \n",
      "Evaluating\n",
      "Starting Epoch 3, Batch 1, Step 5\n",
      "Starting Epoch 3, Batch 2, Step 6\n",
      "Epoch 3 | time = 2s | loss gen = 0.8583 | loss enc = 0.6402 | loss dis = 1.4080 \n",
      "Starting Epoch 4, Batch 1, Step 7\n",
      "Starting Epoch 4, Batch 2, Step 8\n",
      "Epoch 4 | time = 14s | loss gen = 0.8696 | loss enc = 0.5741 | loss dis = 1.9557 \n",
      "Evaluating\n",
      "Starting Epoch 5, Batch 1, Step 9\n",
      "Starting Epoch 5, Batch 2, Step 10\n",
      "Epoch 5 | time = 2s | loss gen = 0.9235 | loss enc = 0.6331 | loss dis = 1.8861 \n",
      "Starting Epoch 6, Batch 1, Step 11\n",
      "Starting Epoch 6, Batch 2, Step 12\n",
      "Epoch 6 | time = 2s | loss gen = 0.9132 | loss enc = 0.6962 | loss dis = 1.8793 \n",
      "Evaluating\n",
      "Starting Epoch 7, Batch 1, Step 13\n",
      "Starting Epoch 7, Batch 2, Step 14\n",
      "Epoch 7 | time = 3s | loss gen = 1.0015 | loss enc = 0.6566 | loss dis = 2.0109 \n",
      "Starting Epoch 8, Batch 1, Step 15\n",
      "Starting Epoch 8, Batch 2, Step 16\n",
      "Epoch 8 | time = 2s | loss gen = 0.9712 | loss enc = 0.6793 | loss dis = 2.0590 \n",
      "Evaluating\n",
      "Starting Epoch 9, Batch 1, Step 17\n",
      "Starting Epoch 9, Batch 2, Step 18\n",
      "Epoch 9 | time = 2s | loss gen = 1.0587 | loss enc = 0.6520 | loss dis = 1.7814 \n",
      "Starting Epoch 10, Batch 1, Step 19\n",
      "Starting Epoch 10, Batch 2, Step 20\n",
      "Epoch 10 | time = 2s | loss gen = 0.6706 | loss enc = 0.6859 | loss dis = 1.8285 \n",
      "Evaluating\n",
      "Starting Epoch 11, Batch 1, Step 21\n",
      "Starting Epoch 11, Batch 2, Step 22\n",
      "Epoch 11 | time = 2s | loss gen = 0.6505 | loss enc = 0.6979 | loss dis = 1.8127 \n",
      "Starting Epoch 12, Batch 1, Step 23\n",
      "Starting Epoch 12, Batch 2, Step 24\n",
      "Epoch 12 | time = 2s | loss gen = 1.0348 | loss enc = 0.6575 | loss dis = 1.7980 \n",
      "Evaluating\n",
      "Starting Epoch 13, Batch 1, Step 25\n",
      "Starting Epoch 13, Batch 2, Step 26\n",
      "Epoch 13 | time = 2s | loss gen = 1.0449 | loss enc = 0.6109 | loss dis = 1.7545 \n",
      "Starting Epoch 14, Batch 1, Step 27\n",
      "Starting Epoch 14, Batch 2, Step 28\n",
      "Epoch 14 | time = 3s | loss gen = 0.9078 | loss enc = 0.5725 | loss dis = 1.9060 \n",
      "Evaluating\n",
      "Starting Epoch 15, Batch 1, Step 29\n",
      "Starting Epoch 15, Batch 2, Step 30\n",
      "Epoch 15 | time = 2s | loss gen = 0.9198 | loss enc = 0.5860 | loss dis = 1.9097 \n",
      "Starting Epoch 16, Batch 1, Step 31\n",
      "Starting Epoch 16, Batch 2, Step 32\n",
      "Epoch 16 | time = 2s | loss gen = 0.8372 | loss enc = 0.6033 | loss dis = 1.8862 \n",
      "Evaluating\n",
      "Starting Epoch 17, Batch 1, Step 33\n",
      "Starting Epoch 17, Batch 2, Step 34\n",
      "Epoch 17 | time = 3s | loss gen = 0.9145 | loss enc = 0.6339 | loss dis = 1.8674 \n",
      "Starting Epoch 18, Batch 1, Step 35\n",
      "Starting Epoch 18, Batch 2, Step 36\n",
      "Epoch 18 | time = 2s | loss gen = 0.9143 | loss enc = 0.5708 | loss dis = 1.7753 \n",
      "Evaluating\n",
      "Starting Epoch 19, Batch 1, Step 37\n",
      "Starting Epoch 19, Batch 2, Step 38\n",
      "Epoch 19 | time = 2s | loss gen = 0.7375 | loss enc = 0.6807 | loss dis = 1.8221 \n",
      "Starting Epoch 20, Batch 1, Step 39\n",
      "Starting Epoch 20, Batch 2, Step 40\n",
      "Epoch 20 | time = 2s | loss gen = 0.8684 | loss enc = 0.6051 | loss dis = 1.6030 \n",
      "Evaluating\n",
      "Starting Epoch 21, Batch 1, Step 41\n",
      "Starting Epoch 21, Batch 2, Step 42\n",
      "Epoch 21 | time = 2s | loss gen = 0.9994 | loss enc = 0.6425 | loss dis = 1.7001 \n",
      "Starting Epoch 22, Batch 1, Step 43\n",
      "Starting Epoch 22, Batch 2, Step 44\n",
      "Epoch 22 | time = 2s | loss gen = 0.8400 | loss enc = 0.5950 | loss dis = 1.7431 \n",
      "Evaluating\n",
      "Starting Epoch 23, Batch 1, Step 45\n",
      "Starting Epoch 23, Batch 2, Step 46\n",
      "Epoch 23 | time = 2s | loss gen = 0.9639 | loss enc = 0.6274 | loss dis = 1.6720 \n",
      "Starting Epoch 24, Batch 1, Step 47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 24, Batch 2, Step 48\n",
      "Epoch 24 | time = 3s | loss gen = 0.9173 | loss enc = 0.5829 | loss dis = 1.7688 \n",
      "Evaluating\n",
      "Starting Epoch 25, Batch 1, Step 49\n",
      "Starting Epoch 25, Batch 2, Step 50\n",
      "Epoch 25 | time = 2s | loss gen = 0.8861 | loss enc = 0.5465 | loss dis = 1.6589 \n",
      "Starting Epoch 26, Batch 1, Step 51\n",
      "Starting Epoch 26, Batch 2, Step 52\n",
      "Epoch 26 | time = 2s | loss gen = 0.8199 | loss enc = 0.6299 | loss dis = 1.7940 \n",
      "Evaluating\n",
      "Starting Epoch 27, Batch 1, Step 53\n",
      "Starting Epoch 27, Batch 2, Step 54\n",
      "Epoch 27 | time = 3s | loss gen = 0.7798 | loss enc = 0.6348 | loss dis = 1.5995 \n",
      "Starting Epoch 28, Batch 1, Step 55\n",
      "Starting Epoch 28, Batch 2, Step 56\n",
      "Epoch 28 | time = 2s | loss gen = 0.8983 | loss enc = 0.6130 | loss dis = 1.7615 \n",
      "Evaluating\n",
      "Starting Epoch 29, Batch 1, Step 57\n",
      "Starting Epoch 29, Batch 2, Step 58\n",
      "Epoch 29 | time = 2s | loss gen = 0.8677 | loss enc = 0.6114 | loss dis = 1.8158 \n",
      "Starting Epoch 30, Batch 1, Step 59\n",
      "Starting Epoch 30, Batch 2, Step 60\n",
      "Epoch 30 | time = 2s | loss gen = 0.9391 | loss enc = 0.6261 | loss dis = 1.7721 \n",
      "Evaluating\n",
      "Starting Epoch 31, Batch 1, Step 61\n",
      "Starting Epoch 31, Batch 2, Step 62\n",
      "Epoch 31 | time = 2s | loss gen = 0.9357 | loss enc = 0.6351 | loss dis = 1.5225 \n",
      "Starting Epoch 32, Batch 1, Step 63\n",
      "Starting Epoch 32, Batch 2, Step 64\n",
      "Epoch 32 | time = 2s | loss gen = 0.8776 | loss enc = 0.5819 | loss dis = 1.6828 \n",
      "Evaluating\n",
      "Starting Epoch 33, Batch 1, Step 65\n",
      "Starting Epoch 33, Batch 2, Step 66\n",
      "Epoch 33 | time = 2s | loss gen = 0.8965 | loss enc = 0.6536 | loss dis = 1.5758 \n",
      "Starting Epoch 34, Batch 1, Step 67\n",
      "Starting Epoch 34, Batch 2, Step 68\n",
      "Epoch 34 | time = 3s | loss gen = 0.8787 | loss enc = 0.5600 | loss dis = 1.6702 \n",
      "Evaluating\n",
      "Starting Epoch 35, Batch 1, Step 69\n",
      "Starting Epoch 35, Batch 2, Step 70\n",
      "Epoch 35 | time = 2s | loss gen = 0.9326 | loss enc = 0.5525 | loss dis = 1.6115 \n",
      "Starting Epoch 36, Batch 1, Step 71\n",
      "Starting Epoch 36, Batch 2, Step 72\n",
      "Epoch 36 | time = 2s | loss gen = 0.8341 | loss enc = 0.6369 | loss dis = 1.7312 \n",
      "Evaluating\n",
      "Starting Epoch 37, Batch 1, Step 73\n",
      "Starting Epoch 37, Batch 2, Step 74\n",
      "Epoch 37 | time = 3s | loss gen = 1.0501 | loss enc = 0.6739 | loss dis = 1.6032 \n",
      "Starting Epoch 38, Batch 1, Step 75\n",
      "Starting Epoch 38, Batch 2, Step 76\n",
      "Epoch 38 | time = 2s | loss gen = 0.9261 | loss enc = 0.6181 | loss dis = 1.8602 \n",
      "Evaluating\n",
      "Starting Epoch 39, Batch 1, Step 77\n",
      "Starting Epoch 39, Batch 2, Step 78\n",
      "Epoch 39 | time = 2s | loss gen = 0.8628 | loss enc = 0.6092 | loss dis = 1.6327 \n",
      "Starting Epoch 40, Batch 1, Step 79\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-23031dde4a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mblowhole_img_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblowhole_img_gt_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mcrack_img_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrack_img_gt_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           \u001b[0mbreak_img_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_img_gt_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m          )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-56bd2837e93d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainx_names, testx_names, blowhole_x_names, blowhole_y_names, crack_x_names, crack_y_names, break_x_names, break_y_names)\u001b[0m\n\u001b[1;32m    304\u001b[0m                                       \u001b[0mloss_discriminator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                                       sum_op_dis],\n\u001b[0;32m--> 306\u001b[0;31m                                      feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0mtrain_loss_dis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    669\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1149\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1222\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    train(\n",
    "          train_img_names, test_img_names, \n",
    "          blowhole_img_names, blowhole_img_gt_names, \n",
    "          crack_img_names, crack_img_gt_names,\n",
    "          break_img_names, break_img_gt_names\n",
    "         )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
