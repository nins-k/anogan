{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_components import *\n",
    "from data_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths for unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'datasets/magnetic_tiles'\n",
    "\n",
    "free_images_dir = 'MT_Free/Imgs'\n",
    "blowhole_images_dir = 'MT_Blowhole/Imgs'\n",
    "break_images_dir = 'MT_Break/Imgs'\n",
    "crack_images_dir = 'MT_Crack/Imgs'\n",
    "\n",
    "new_dataset_dir = 'datasets/rescaled_magnetic_tiles'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get names of unprocessed images and their gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_img_names, free_img_gt_names = get_img_and_gt_names(dataset_dir, free_images_dir)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(dataset_dir, crack_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and save defect images and their gt (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 57, Created 213, Conversion Rate 3.74\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(crack_img_names, \n",
    "                                crack_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, crack_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 115, Created 427, Conversion Rate 3.71\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(blowhole_img_names, \n",
    "                                blowhole_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, blowhole_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 85, Created 278, Conversion Rate 3.27\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(break_img_names, \n",
    "                                break_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, break_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and save free images (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 952, Created 2960, Conversion Rate 3.11\n"
     ]
    }
   ],
   "source": [
    "inp, op = scale_and_random_crop(free_img_names, \n",
    "                                scale_size=256, \n",
    "                                random_crop_range=50, \n",
    "                                image_count=None, \n",
    "                                save_dir=os.path.join(new_dataset_dir, free_images_dir)\n",
    "                               )\n",
    "\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op,\n",
    "                                                                   op/inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load defect images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed defect images (256x256)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(new_dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(new_dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(new_dataset_dir, crack_images_dir)\n",
    "\n",
    "blowhole_images_set = load_and_normalize(blowhole_img_names)\n",
    "blowhole_gt_images_set = load_and_normalize(blowhole_img_gt_names)\n",
    "\n",
    "break_images_set = load_and_normalize(break_img_names)\n",
    "break_gt_images_set = load_and_normalize(break_img_gt_names)\n",
    "\n",
    "crack_images_set = load_and_normalize(crack_img_names)\n",
    "crack_gt_images_set = load_and_normalize(crack_img_gt_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load free images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Training Images: 2560, Free Test Images: 400\n"
     ]
    }
   ],
   "source": [
    "free_img_names = glob.glob(os.path.join(new_dataset_dir, free_images_dir, \"*.jpg\"))\n",
    "\n",
    "train_img_names, test_img_names = train_test_split(free_img_names, train_size=2560)\n",
    "\n",
    "train_images = load_and_normalize(train_img_names)\n",
    "test_images = load_and_normalize(test_img_names)\n",
    "\n",
    "print(\"Free Training Images: {}, Free Test Images: {}\".format(len(train_images), len(test_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final image count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Training Images: 2560\n",
      "Free Test Images: 400\n",
      "Blowhole Test Images: 427\n",
      "Break Test Images: 278\n",
      "Crack Test Images: 213\n"
     ]
    }
   ],
   "source": [
    "print(\"Free Training Images: {}\\nFree Test Images: {}\\nBlowhole Test Images: {}\\\n",
    "\\nBreak Test Images: {}\\nCrack Test Images: {}\".format(\n",
    "                                                                                          len(train_images),\n",
    "                                                                                          len(test_images),\n",
    "                                                                                          len(blowhole_images_set),\n",
    "                                                                                          len(break_images_set),\n",
    "                                                                                          len(crack_images_set)\n",
    "                                                                                         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_dir(checkpoint_dir):\n",
    "    r = glob.glob(os.path.join(checkpoint_dir, \"logs*\"))\n",
    "    log_dir_name = os.path.join(checkpoint_dir, \"logs{}\".format(str(len(r)+1)))\n",
    "                  \n",
    "    return log_dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainx, testx, blowhole_x, blowhole_y, crack_x, crack_y, break_x, break_y):\n",
    "    \n",
    "    penultimate_layer_units = 1024\n",
    "    latent_dimensions = 200\n",
    "    batch_size = 16\n",
    "    FREQ_PRINT = 80\n",
    "    learning_rate = 0.0002\n",
    "    nb_epochs = 500\n",
    "    freq_epoch_test = 5\n",
    "    num_test_images = 32\n",
    "    \n",
    "    # Image input placeholder\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # Ground truth input placeholder\n",
    "    gt = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # mean and variance of the free image scores\n",
    "    mean_inp = tf.placeholder(dtype=tf.float32)\n",
    "    var_inp = tf.placeholder(dtype=tf.float32)\n",
    "    \n",
    "    # Training mode placeholder\n",
    "    training_mode = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_real_image = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=False, training_mode=True)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        z = tf.random_normal([batch_size, latent_dimensions])\n",
    "        generated_image = get_generator_model(z, reuse=False, training_mode=True)\n",
    "        regenerated_real_image = get_generator_model(encoding_real_image, reuse=True, training_mode=False)\n",
    "    \n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake, dis_fake_penultimate_layer = get_discriminator_model(generated_image, z, reuse=False, \n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "        discriminator_real, dis_real_penultimate_layer = get_discriminator_model(x, encoding_real_image, reuse=True,\n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "    \n",
    "    # Prepare labels for the loss functions\n",
    "    with tf.variable_scope('labels'):\n",
    "        \n",
    "        # Step 1\n",
    "        # Set swapped labels\n",
    "        labels_dis_enc = tf.zeros_like(discriminator_real)\n",
    "        labels_dis_gen = tf.ones_like(discriminator_fake)\n",
    "        labels_gen = tf.zeros_like(discriminator_fake)\n",
    "        labels_enc = tf.ones_like(discriminator_real)\n",
    "        \n",
    "        # Step 2\n",
    "        # Create soft labels for the discriminator\n",
    "        random_soft = tf.random_uniform(shape=(tf.shape(labels_dis_enc)), minval=0.0, maxval=0.1)\n",
    "        soft_labels_dis_enc = tf.add(labels_dis_enc, random_soft)\n",
    "        soft_labels_dis_gen = tf.subtract(labels_dis_gen, random_soft)\n",
    "\n",
    "        # Step 3\n",
    "        # With a low chance, assign noisy (swapped) labels\n",
    "        random_flip = tf.ones_like(labels_dis_enc) * tf.random_uniform(shape=(1,), minval=0, maxval=1)\n",
    "        mask = random_flip >= 0.05\n",
    "        labels_dis_enc = tf.where(mask, soft_labels_dis_enc, soft_labels_dis_gen)\n",
    "        labels_dis_gen = tf.where(mask, soft_labels_dis_gen, soft_labels_dis_enc)\n",
    "    \n",
    "    # Loss Functions\n",
    "    with tf.variable_scope('loss_functions'):\n",
    "        loss_dis_enc = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_dis_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "        loss_dis_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(discriminator_fake),\n",
    "                                                                              logits=discriminator_fake))\n",
    "        loss_discriminator = loss_dis_gen + loss_dis_enc\n",
    "        # generator\n",
    "        loss_generator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_gen,\n",
    "                                                                                logits=discriminator_fake))\n",
    "        # encoder\n",
    "        loss_encoder = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "    with tf.name_scope('optimizers'):\n",
    "        # control op dependencies for batch norm and trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        dvars = [var for var in tvars if 'discriminator_model' in var.name]\n",
    "        gvars = [var for var in tvars if 'generator_model' in var.name]\n",
    "        evars = [var for var in tvars if 'encoder_model' in var.name]\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        update_ops_gen = [x for x in update_ops if ('generator_model' in x.name)]\n",
    "        update_ops_enc = [x for x in update_ops if ('encoder_model' in x.name)]\n",
    "        update_ops_dis = [x for x in update_ops if ('discriminator_model' in x.name)]\n",
    "\n",
    "        optimizer_dis = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='dis_optimizer')\n",
    "        optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='gen_optimizer')\n",
    "        optimizer_enc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='enc_optimizer')\n",
    "\n",
    "        with tf.control_dependencies(update_ops_gen):\n",
    "            gen_op = optimizer_gen.minimize(loss_generator, var_list=gvars)\n",
    "        with tf.control_dependencies(update_ops_enc):\n",
    "            enc_op = optimizer_enc.minimize(loss_encoder, var_list=evars, global_step=tf.train.get_or_create_global_step())\n",
    "        with tf.control_dependencies(update_ops_dis):\n",
    "            dis_op = optimizer_dis.minimize(loss_discriminator, var_list=dvars)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        with tf.name_scope('discriminator'):\n",
    "            tf.summary.scalar('loss_total', loss_discriminator, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_enc', loss_dis_enc, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_gen', loss_dis_gen, ['dis'])\n",
    "\n",
    "        with tf.name_scope('generator'):\n",
    "            tf.summary.scalar('loss_generator', loss_generator, ['gen'])\n",
    "            tf.summary.scalar('loss_encoder', loss_encoder, ['gen'])\n",
    "\n",
    "    with tf.name_scope('train_img_regen'):\n",
    "        for p in range(4):\n",
    "            tf.summary.image('img_{}_regen'.format(p+1), regenerated_real_image[p:p+1,:,:,:], 1, ['image'])\n",
    "            tf.summary.image('img_{}_input'.format(p+1), x[p:p+1,:,:,:], 1, ['image'])\n",
    "\n",
    "    sum_op_dis = tf.summary.merge_all('dis')\n",
    "    sum_op_gen = tf.summary.merge_all('gen')\n",
    "    sum_op_im = tf.summary.merge_all('image')\n",
    "\n",
    "        \n",
    "    '''\n",
    "    ----------------------------------------TRAINING OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "        \n",
    "    # TESTING GRAPH\n",
    "\n",
    "\n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_test = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=True, training_mode=False)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        regenerated_image_test = get_generator_model(encoding_test, reuse=True, training_mode=False)\n",
    "\n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake_test, dis_fake_penultimate_layer_test = get_discriminator_model(regenerated_image_test, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True, \n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "        discriminator_real_test, dis_real_penultimate_layer_test = get_discriminator_model(x, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True,\n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "    with tf.name_scope('testing'):\n",
    "        with tf.variable_scope('reconstruction_loss'):\n",
    "            delta = x - regenerated_image_test\n",
    "            delta_flat = tf.layers.flatten(delta)\n",
    "            gen_score = tf.norm(delta_flat, ord='euclidean', axis=1,\n",
    "                              keep_dims=False, name='epsilon')\n",
    "\n",
    "        with tf.variable_scope('discriminator_loss'):\n",
    "            fm = dis_real_penultimate_layer_test - dis_fake_penultimate_layer_test\n",
    "            fm = tf.contrib.layers.flatten(fm)\n",
    "            dis_score = tf.norm(fm, ord='euclidean', axis=1,\n",
    "                             keep_dims=False, name='d_loss')\n",
    "            dis_score = tf.squeeze(dis_score)\n",
    "\n",
    "            \n",
    "        weight1, weight2, weight3, weight4, weight5 = 0.1, 0.2, 0.3, 0.4, 0.5 \n",
    "        \n",
    "        with tf.variable_scope('score'):\n",
    "            mean_score1 = tf.reduce_mean((1 - weight1) * gen_score + weight1 * dis_score)\n",
    "            mean_score2 = tf.reduce_mean((1 - weight2) * gen_score + weight2 * dis_score)\n",
    "            mean_score3 = tf.reduce_mean((1 - weight3) * gen_score + weight3 * dis_score)\n",
    "            mean_score4 = tf.reduce_mean((1 - weight4) * gen_score + weight4 * dis_score)\n",
    "            mean_score5 = tf.reduce_mean((1 - weight5) * gen_score + weight5 * dis_score)\n",
    "            \n",
    "\n",
    "    with tf.name_scope('test_anomaly_score'):\n",
    "        tf.summary.scalar(\"mean_score_w=0.1\", mean_score1, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.2\", mean_score2, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.3\", mean_score3, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.4\", mean_score4, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.5\", mean_score5, ['scr'])\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('accuracy'):\n",
    "        \n",
    "        # For defect accuracy calculation\n",
    "        all_test_scores = (1 - weight1) * gen_score + weight1 * dis_score\n",
    "        free_thresh_0 = mean_inp\n",
    "        free_thresh_1 = mean_inp + tf.sqrt(var_inp)\n",
    "        free_thresh_2 = mean_inp + 2* tf.sqrt(var_inp)\n",
    "        free_thresh_3 = mean_inp + 3 * tf.sqrt(var_inp)\n",
    "        \n",
    "        bool_list_0 = tf.greater_equal(all_test_scores, free_thresh_0)\n",
    "        test_acc_0 = tf.reduce_sum(tf.cast(bool_list_0, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_1 = tf.greater_equal(all_test_scores, free_thresh_1)\n",
    "        test_acc_1 = tf.reduce_sum(tf.cast(bool_list_1, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_2 = tf.greater_equal(all_test_scores, free_thresh_2)\n",
    "        test_acc_2 = tf.reduce_sum(tf.cast(bool_list_2, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_3 = tf.greater_equal(all_test_scores, free_thresh_3)\n",
    "        test_acc_3 = tf.reduce_sum(tf.cast(bool_list_3, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        # For calculating optimal anomaly score based on free image scores\n",
    "        mean, var = tf.nn.moments(all_test_scores, axes=[0])\n",
    "\n",
    "        \n",
    "    \n",
    "    with tf.name_scope('test_accuracy'):\n",
    "        \n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_0', test_acc_0, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_1', test_acc_1, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_2', test_acc_2, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_3', test_acc_3, ['test_acc'])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('test_img_regen'):\n",
    "        for p in range(2):\n",
    "            tf.summary.image('{}_0_input'.format(p+1), x[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_1_regen'.format(p+1), regenerated_image_test[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_2_ground_truth'.format(p+1), gt[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_3_difference'.format(p+1), delta[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            \n",
    "            \n",
    "    sum_op_scr = tf.summary.merge_all('scr')\n",
    "    sum_op_t_img = tf.summary.merge_all('t_image')\n",
    "    sum_op_test_acc = tf.summary.merge_all('test_acc')\n",
    "    \n",
    "    gs = tf.train.get_global_step()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ----------------------------------------TEST OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    # TRAINING\n",
    "\n",
    "    checkpoint_dir = \"train/train01/\"\n",
    "    summary_dir = get_summary_dir(checkpoint_dir)\n",
    "    \n",
    "    free_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"free\"))\n",
    "    blowhole_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"blowhole\"))\n",
    "    crack_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"crack\"))\n",
    "    break_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"break\"))\n",
    "\n",
    "\n",
    "    step_saver =tf.train.CheckpointSaverHook(checkpoint_dir=checkpoint_dir, save_steps=1600, save_secs=None)\n",
    "\n",
    "    summary_saver = tf.train.SummarySaverHook(save_steps=8,\n",
    "                                              save_secs=None,\n",
    "                                              output_dir=summary_dir, \n",
    "                                              summary_op=[sum_op_dis, sum_op_gen, sum_op_im]\n",
    "                                             )\n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    \n",
    "    mnt = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=[step_saver, summary_saver])\n",
    "\n",
    "                                   \n",
    "\n",
    "    with mnt as sess:\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        train_batch = 0\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "        while not mnt.should_stop() and epoch < nb_epochs:\n",
    "\n",
    "            begin = time.time()\n",
    "            nr_batches_train = int(trainx.shape[0] / batch_size)\n",
    "\n",
    "            # shuffling dataset\n",
    "            trainx = trainx[np.random.permutation(trainx.shape[0])] \n",
    "            train_loss_dis, train_loss_gen, train_loss_enc = [0, 0, 0]\n",
    "\n",
    "            # training\n",
    "            for t in range(nr_batches_train):\n",
    "\n",
    "                print(\"Starting Epoch {}, Batch {}, Step {}\".format(epoch+1, t+1, step+1))     \n",
    "                ran_from = t * batch_size\n",
    "                ran_to = (t + 1) * batch_size\n",
    "\n",
    "                # train discriminator\n",
    "                feed_dict = {x: trainx[ran_from:ran_to],\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "\n",
    "                _, ld, sm = sess.run([dis_op,\n",
    "                                      loss_discriminator,\n",
    "                                      sum_op_dis],\n",
    "                                     feed_dict=feed_dict)\n",
    "                train_loss_dis += ld\n",
    "\n",
    "                # train generator and encoder\n",
    "                feed_dict = {x: trainx[ran_from:ran_to],\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "                _,_, le, lg, sm = sess.run([gen_op,\n",
    "                                            enc_op,\n",
    "                                            loss_encoder,\n",
    "                                            loss_generator,\n",
    "                                            sum_op_gen],\n",
    "                                           feed_dict=feed_dict)\n",
    "                train_loss_gen += lg\n",
    "                train_loss_enc += le\n",
    "\n",
    "\n",
    "                train_batch += 1\n",
    "                step+=1\n",
    "\n",
    "            train_loss_gen /= nr_batches_train\n",
    "            train_loss_enc /= nr_batches_train\n",
    "            train_loss_dis /= nr_batches_train\n",
    "\n",
    "            print(\"Epoch %d | time = %ds | loss gen = %.4f | loss enc = %.4f | loss dis = %.4f \"\n",
    "                  % (epoch+1, time.time() - begin, train_loss_gen, train_loss_enc, train_loss_dis))\n",
    "            \n",
    "            # Inspect reconstruction\n",
    "            if (epoch+1) % freq_epoch_test == 0:  \n",
    "                    t= np.random.randint(0,trainx.shape[0]-batch_size)\n",
    "                    ran_from = t\n",
    "                    ran_to = t + 4\n",
    "                    sm = sess.run(sum_op_im, feed_dict={x: trainx[ran_from:ran_to],training_mode: False})\n",
    "            \n",
    "            # Test\n",
    "            \n",
    "            if (epoch+1) % freq_epoch_test == 0:\n",
    "                print(\"Evaluating\")\n",
    "                \n",
    "                # Shuffle\n",
    "                testx = testx[np.random.permutation(testx.shape[0])]\n",
    "                \n",
    "                blowhole_indices = np.random.permutation(blowhole_x.shape[0])\n",
    "                blowhole_x = blowhole_x[blowhole_indices]\n",
    "                blowhole_y = blowhole_y[blowhole_indices]\n",
    "                \n",
    "                break_indices= np.random.permutation(break_x.shape[0])\n",
    "                break_x = break_x[break_indices]\n",
    "                break_y = break_y[break_indices]\n",
    "                \n",
    "                crack_indices = np.random.permutation(crack_x.shape[0])\n",
    "                crack_x = crack_x[crack_indices]\n",
    "                crack_y = crack_y[crack_indices]\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Free Test\n",
    "                free_score_summary, free_t_img_summary, current_step, mean_score, var_score = \\\n",
    "                                                                            sess.run([sum_op_scr, sum_op_t_img, gs,\n",
    "                                                                                     mean, var], \n",
    "                                                                            feed_dict={x: testx[0:num_test_images],\n",
    "                                                                                       gt: np.zeros_like(testx),\n",
    "                                                                                       mean_inp: 0,\n",
    "                                                                                       var_inp: 0,\n",
    "                                                                                       training_mode: False})\n",
    "                free_writer.add_summary(free_score_summary, current_step)\n",
    "                free_writer.add_summary(free_t_img_summary, current_step)\n",
    "                free_writer.flush()\n",
    "                \n",
    "\n",
    "                # Blowhole\n",
    "                blowhole_score_summary, blowhole_t_img_summary, blowhole_acc_summary = \\\n",
    "                                                                          sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                          feed_dict={x: blowhole_x[0:num_test_images],\n",
    "                                                                                     gt: blowhole_y[0:num_test_images],\n",
    "                                                                                     mean_inp: mean_score,\n",
    "                                                                                     var_inp: var_score,\n",
    "                                                                                     training_mode: False})\n",
    "                blowhole_writer.add_summary(blowhole_score_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_t_img_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_acc_summary, current_step)\n",
    "                blowhole_writer.flush()\n",
    "                \n",
    "                # Crack\n",
    "                crack_score_summary, crack_t_img_summary, crack_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: crack_x[0:num_test_images], \n",
    "                                                                               gt: crack_y[0:num_test_images],\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                crack_writer.add_summary(crack_score_summary, current_step)\n",
    "                crack_writer.add_summary(crack_t_img_summary, current_step)\n",
    "                crack_writer.add_summary(crack_acc_summary, current_step)\n",
    "                crack_writer.flush()\n",
    "                    \n",
    "                # Break\n",
    "                break_score_summary, break_t_img_summary, break_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: break_x[0:num_test_images],\n",
    "                                                                               gt: break_y[0:num_test_images],\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                break_writer.add_summary(break_score_summary, current_step)\n",
    "                break_writer.add_summary(break_t_img_summary, current_step)\n",
    "                break_writer.add_summary(break_acc_summary, current_step)\n",
    "                break_writer.flush()\n",
    "                \n",
    "            \n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (4, 200)\n",
      "Output shape of layer_01 is (4, 8, 8, 1024)\n",
      "Output shape of layer_02 is (4, 16, 16, 512)\n",
      "Output shape of layer_03 is (4, 32, 32, 256)\n",
      "Output shape of layer_04 is (4, 64, 64, 128)\n",
      "Output shape of layer_05 is (4, 128, 128, 64)\n",
      "Output shape of layer_06 is (4, 256, 256, 3)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (4, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (4, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (4, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (4, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (4, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (4, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (4, 200)\n",
      "Output shape of z_layer_01 is (4, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (4, 66560)\n",
      "Output shape of xz_layer_01 is (4, 1024)\n",
      "Output shape of xz_layer_02 is (4, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "INFO:tensorflow:Summary name mean_score_w=0.1 is illegal; using mean_score_w_0.1 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.2 is illegal; using mean_score_w_0.2 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.3 is illegal; using mean_score_w_0.3 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.4 is illegal; using mean_score_w_0.4 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.5 is illegal; using mean_score_w_0.5 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_0 is illegal; using threshold_with_w_0.1__stddev_0 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_1 is illegal; using threshold_with_w_0.1__stddev_1 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_2 is illegal; using threshold_with_w_0.1__stddev_2 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_3 is illegal; using threshold_with_w_0.1__stddev_3 instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train01/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/train01/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/train01/model.ckpt.\n",
      "Starting Epoch 1, Batch 1, Step 1\n",
      "Starting Epoch 1, Batch 2, Step 2\n",
      "Starting Epoch 1, Batch 3, Step 3\n",
      "Starting Epoch 1, Batch 4, Step 4\n",
      "Epoch 1 | time = 10s | loss gen = 0.9133 | loss enc = 0.6174 | loss dis = 1.8663 \n",
      "Starting Epoch 2, Batch 1, Step 5\n",
      "Starting Epoch 2, Batch 2, Step 6\n",
      "Starting Epoch 2, Batch 3, Step 7\n",
      "Starting Epoch 2, Batch 4, Step 8\n",
      "Epoch 2 | time = 2s | loss gen = 0.8736 | loss enc = 0.5100 | loss dis = 1.9353 \n",
      "Evaluating\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ae8a4b13d8cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     train(train_images, test_images, blowhole_images_set, blowhole_gt_images_set, crack_images_set, crack_gt_images_set,\n\u001b[0;32m----> 3\u001b[0;31m          break_images_set, break_gt_images_set)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-84e4abcc2004>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainx, testx, blowhole_x, blowhole_y, crack_x, crack_y, break_x, break_y)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;31m# Shuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0mtestx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m                 \u001b[0mblowhole_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblowhole_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    train(train_images, test_images, blowhole_images_set, blowhole_gt_images_set, crack_images_set, crack_gt_images_set,\n",
    "         break_images_set, break_gt_images_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
