{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_components import *\n",
    "from data_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths for unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'datasets/magnetic_tiles'\n",
    "\n",
    "free_images_dir = 'MT_Free/Imgs'\n",
    "blowhole_images_dir = 'MT_Blowhole/Imgs'\n",
    "break_images_dir = 'MT_Break/Imgs'\n",
    "crack_images_dir = 'MT_Crack/Imgs'\n",
    "\n",
    "new_dataset_dir = 'datasets/rescaled_magnetic_tiles'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get names of unprocessed images and their gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_img_names, free_img_gt_names = get_img_and_gt_names(dataset_dir, free_images_dir)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(dataset_dir, crack_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and save defect images and their gt (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 57, Created 213, Conversion Rate 3.74\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(crack_img_names, \n",
    "                                crack_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, crack_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 115, Created 427, Conversion Rate 3.71\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(blowhole_img_names, \n",
    "                                blowhole_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, blowhole_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 85, Created 278, Conversion Rate 3.27\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(break_img_names, \n",
    "                                break_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, break_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and save free images (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 952, Created 2960, Conversion Rate 3.11\n"
     ]
    }
   ],
   "source": [
    "inp, op = scale_and_random_crop(free_img_names, \n",
    "                                scale_size=256, \n",
    "                                random_crop_range=50, \n",
    "                                image_count=None, \n",
    "                                save_dir=os.path.join(new_dataset_dir, free_images_dir)\n",
    "                               )\n",
    "\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op,\n",
    "                                                                   op/inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load defect images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed defect images (256x256)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(new_dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(new_dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(new_dataset_dir, crack_images_dir)\n",
    "\n",
    "blowhole_images_set = load_and_normalize(blowhole_img_names)\n",
    "blowhole_gt_images_set = load_and_normalize(blowhole_img_gt_names)\n",
    "\n",
    "break_images_set = load_and_normalize(break_img_names)\n",
    "break_gt_images_set = load_and_normalize(break_img_gt_names)\n",
    "\n",
    "crack_images_set = load_and_normalize(crack_img_names)\n",
    "crack_gt_images_set = load_and_normalize(crack_img_gt_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load free images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Training Images: 2560, Free Test Images: 400\n"
     ]
    }
   ],
   "source": [
    "free_img_names = glob.glob(os.path.join(new_dataset_dir, free_images_dir, \"*.jpg\"))\n",
    "\n",
    "train_img_names, test_img_names = train_test_split(free_img_names, train_size=2560)\n",
    "\n",
    "train_images = load_and_normalize(train_img_names)\n",
    "test_images = load_and_normalize(test_img_names)\n",
    "\n",
    "print(\"Free Training Images: {}, Free Test Images: {}\".format(len(train_images), len(test_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final image count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Training Images: 2560\n",
      "Free Test Images: 400\n",
      "Blowhole Test Images: 427\n",
      "Break Test Images: 278\n",
      "Crack Test Images: 213\n"
     ]
    }
   ],
   "source": [
    "print(\"Free Training Images: {}\\nFree Test Images: {}\\nBlowhole Test Images: {}\\\n",
    "\\nBreak Test Images: {}\\nCrack Test Images: {}\".format(\n",
    "                                                                                          len(train_images),\n",
    "                                                                                          len(test_images),\n",
    "                                                                                          len(blowhole_images_set),\n",
    "                                                                                          len(break_images_set),\n",
    "                                                                                          len(crack_images_set)\n",
    "                                                                                         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_dir(checkpoint_dir):\n",
    "    r = glob.glob(os.path.join(checkpoint_dir, \"logs*\"))\n",
    "    log_dir_name = os.path.join(checkpoint_dir, \"logs{}\".format(str(len(r)+1)))\n",
    "                  \n",
    "    return log_dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainx, testx, blowhole_x, blowhole_y, crack_x, crack_y, break_x, break_y):\n",
    "    \n",
    "    trainx = trainx[:16]\n",
    "    penultimate_layer_units = 1024\n",
    "    latent_dimensions = 200\n",
    "    batch_size = 4\n",
    "    FREQ_PRINT = 80\n",
    "    learning_rate = 0.0002\n",
    "    nb_epochs = 500\n",
    "    freq_epoch_test = 2\n",
    "    num_test_images = 4\n",
    "    \n",
    "    # Image input placeholder\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # Ground truth input placeholder\n",
    "    gt = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # mean and variance of the free image scores\n",
    "    mean_inp = tf.placeholder(dtype=tf.float32)\n",
    "    var_inp = tf.placeholder(dtype=tf.float32)\n",
    "    \n",
    "    # Training mode placeholder\n",
    "    training_mode = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_real_image = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=False, training_mode=True)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        z = tf.random_normal([batch_size, latent_dimensions])\n",
    "        generated_image = get_generator_model(z, reuse=False, training_mode=True)\n",
    "        regenerated_real_image = get_generator_model(encoding_real_image, reuse=True, training_mode=False)\n",
    "    \n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake, dis_fake_penultimate_layer = get_discriminator_model(generated_image, z, reuse=False, \n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "        discriminator_real, dis_real_penultimate_layer = get_discriminator_model(x, encoding_real_image, reuse=True,\n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "    \n",
    "    # Prepare labels for the loss functions\n",
    "    with tf.variable_scope('labels'):\n",
    "        \n",
    "        # Step 1\n",
    "        # Set swapped labels\n",
    "        labels_dis_enc = tf.zeros_like(discriminator_real)\n",
    "        labels_dis_gen = tf.ones_like(discriminator_fake)\n",
    "        labels_gen = tf.zeros_like(discriminator_fake)\n",
    "        labels_enc = tf.ones_like(discriminator_real)\n",
    "        \n",
    "        # Step 2\n",
    "        # Create soft labels for the discriminator\n",
    "        random_soft = tf.random_uniform(shape=(tf.shape(labels_dis_enc)), minval=0.0, maxval=0.1)\n",
    "        soft_labels_dis_enc = tf.add(labels_dis_enc, random_soft)\n",
    "        soft_labels_dis_gen = tf.subtract(labels_dis_gen, random_soft)\n",
    "\n",
    "        # Step 3\n",
    "        # With a low chance, assign noisy (swapped) labels\n",
    "        random_flip = tf.ones_like(labels_dis_enc) * tf.random_uniform(shape=(1,), minval=0, maxval=1)\n",
    "        mask = random_flip >= 0.05\n",
    "        labels_dis_enc = tf.where(mask, soft_labels_dis_enc, soft_labels_dis_gen)\n",
    "        labels_dis_gen = tf.where(mask, soft_labels_dis_gen, soft_labels_dis_enc)\n",
    "    \n",
    "    # Loss Functions\n",
    "    with tf.variable_scope('loss_functions'):\n",
    "        loss_dis_enc = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_dis_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "        loss_dis_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(discriminator_fake),\n",
    "                                                                              logits=discriminator_fake))\n",
    "        loss_discriminator = loss_dis_gen + loss_dis_enc\n",
    "        # generator\n",
    "        loss_generator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_gen,\n",
    "                                                                                logits=discriminator_fake))\n",
    "        # encoder\n",
    "        loss_encoder = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "    with tf.name_scope('optimizers'):\n",
    "        # control op dependencies for batch norm and trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        dvars = [var for var in tvars if 'discriminator_model' in var.name]\n",
    "        gvars = [var for var in tvars if 'generator_model' in var.name]\n",
    "        evars = [var for var in tvars if 'encoder_model' in var.name]\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        update_ops_gen = [x for x in update_ops if ('generator_model' in x.name)]\n",
    "        update_ops_enc = [x for x in update_ops if ('encoder_model' in x.name)]\n",
    "        update_ops_dis = [x for x in update_ops if ('discriminator_model' in x.name)]\n",
    "\n",
    "        optimizer_dis = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='dis_optimizer')\n",
    "        optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='gen_optimizer')\n",
    "        optimizer_enc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='enc_optimizer')\n",
    "\n",
    "        with tf.control_dependencies(update_ops_gen):\n",
    "            gen_op = optimizer_gen.minimize(loss_generator, var_list=gvars)\n",
    "        with tf.control_dependencies(update_ops_enc):\n",
    "            enc_op = optimizer_enc.minimize(loss_encoder, var_list=evars, global_step=tf.train.get_or_create_global_step())\n",
    "        with tf.control_dependencies(update_ops_dis):\n",
    "            dis_op = optimizer_dis.minimize(loss_discriminator, var_list=dvars)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        with tf.name_scope('discriminator'):\n",
    "            tf.summary.scalar('loss_total', loss_discriminator, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_enc', loss_dis_enc, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_gen', loss_dis_gen, ['dis'])\n",
    "\n",
    "        with tf.name_scope('generator'):\n",
    "            tf.summary.scalar('loss_generator', loss_generator, ['gen'])\n",
    "            tf.summary.scalar('loss_encoder', loss_encoder, ['gen'])\n",
    "\n",
    "    with tf.name_scope('train_img_regen'):\n",
    "        for p in range(4):\n",
    "            tf.summary.image('img_{}_regen'.format(p+1), regenerated_real_image[p:p+1,:,:,:], 1, ['image'])\n",
    "            tf.summary.image('img_{}_input'.format(p+1), x[p:p+1,:,:,:], 1, ['image'])\n",
    "\n",
    "    sum_op_dis = tf.summary.merge_all('dis')\n",
    "    sum_op_gen = tf.summary.merge_all('gen')\n",
    "    sum_op_im = tf.summary.merge_all('image')\n",
    "\n",
    "        \n",
    "    '''\n",
    "    ----------------------------------------TRAINING OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "        \n",
    "    # TESTING GRAPH\n",
    "\n",
    "\n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_test = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=True, training_mode=False)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        regenerated_image_test = get_generator_model(encoding_test, reuse=True, training_mode=False)\n",
    "\n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake_test, dis_fake_penultimate_layer_test = get_discriminator_model(regenerated_image_test, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True, \n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "        discriminator_real_test, dis_real_penultimate_layer_test = get_discriminator_model(x, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True,\n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "    with tf.name_scope('testing'):\n",
    "        with tf.variable_scope('reconstruction_loss'):\n",
    "            delta = x - regenerated_image_test\n",
    "            delta_flat = tf.layers.flatten(delta)\n",
    "            gen_score = tf.norm(delta_flat, ord='euclidean', axis=1,\n",
    "                              keep_dims=False, name='epsilon')\n",
    "\n",
    "        with tf.variable_scope('discriminator_loss'):\n",
    "            fm = dis_real_penultimate_layer_test - dis_fake_penultimate_layer_test\n",
    "            fm = tf.contrib.layers.flatten(fm)\n",
    "            dis_score = tf.norm(fm, ord='euclidean', axis=1,\n",
    "                             keep_dims=False, name='d_loss')\n",
    "            dis_score = tf.squeeze(dis_score)\n",
    "\n",
    "            \n",
    "        weight1, weight2, weight3, weight4, weight5 = 0.1, 0.2, 0.3, 0.4, 0.5 \n",
    "        \n",
    "        with tf.variable_scope('score'):\n",
    "            mean_score1 = tf.reduce_mean((1 - weight1) * gen_score + weight1 * dis_score)\n",
    "            mean_score2 = tf.reduce_mean((1 - weight2) * gen_score + weight2 * dis_score)\n",
    "            mean_score3 = tf.reduce_mean((1 - weight3) * gen_score + weight3 * dis_score)\n",
    "            mean_score4 = tf.reduce_mean((1 - weight4) * gen_score + weight4 * dis_score)\n",
    "            mean_score5 = tf.reduce_mean((1 - weight5) * gen_score + weight5 * dis_score)\n",
    "            \n",
    "\n",
    "    with tf.name_scope('test_anomaly_score'):\n",
    "        tf.summary.scalar(\"mean_score_w=0.1\", mean_score1, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.2\", mean_score2, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.3\", mean_score3, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.4\", mean_score4, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.5\", mean_score5, ['scr'])\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('accuracy'):\n",
    "        \n",
    "        # For defect accuracy calculation\n",
    "        all_test_scores = (1 - weight1) * gen_score + weight1 * dis_score\n",
    "        free_thresh_0 = mean_inp\n",
    "        free_thresh_1 = mean_inp + tf.sqrt(var_inp)\n",
    "        free_thresh_2 = mean_inp + 2* tf.sqrt(var_inp)\n",
    "        free_thresh_3 = mean_inp + 3 * tf.sqrt(var_inp)\n",
    "        \n",
    "        bool_list_0 = tf.greater_equal(all_test_scores, free_thresh_0)\n",
    "        test_acc_0 = tf.reduce_sum(tf.cast(bool_list_0, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_1 = tf.greater_equal(all_test_scores, free_thresh_1)\n",
    "        test_acc_1 = tf.reduce_sum(tf.cast(bool_list_1, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_2 = tf.greater_equal(all_test_scores, free_thresh_2)\n",
    "        test_acc_2 = tf.reduce_sum(tf.cast(bool_list_2, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_3 = tf.greater_equal(all_test_scores, free_thresh_3)\n",
    "        test_acc_3 = tf.reduce_sum(tf.cast(bool_list_3, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        # For calculating optimal anomaly score based on free image scores\n",
    "        mean, var = tf.nn.moments(all_test_scores, axes=[0])\n",
    "        \n",
    "    \n",
    "    with tf.name_scope('test_accuracy'):\n",
    "        \n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_0', test_acc_0, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_1', test_acc_1, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_2', test_acc_2, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_3', test_acc_3, ['test_acc'])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('test_img_regen'):\n",
    "        for p in range(2):\n",
    "            tf.summary.image('{}_0_input'.format(p+1), x[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_1_regen'.format(p+1), regenerated_image_test[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_2_ground_truth'.format(p+1), gt[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_3_difference'.format(p+1), delta[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            \n",
    "            \n",
    "    sum_op_scr = tf.summary.merge_all('scr')\n",
    "    sum_op_t_img = tf.summary.merge_all('t_image')\n",
    "    sum_op_test_acc = tf.summary.merge_all('test_acc')\n",
    "    \n",
    "    gs = tf.train.get_global_step()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ----------------------------------------TEST OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    # TRAINING\n",
    "\n",
    "    checkpoint_dir = \"train/train01/\"\n",
    "    summary_dir = get_summary_dir(checkpoint_dir)\n",
    "    \n",
    "    free_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"free\"))\n",
    "    blowhole_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"blowhole\"))\n",
    "    crack_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"crack\"))\n",
    "    break_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"break\"))\n",
    "\n",
    "\n",
    "    step_saver =tf.train.CheckpointSaverHook(checkpoint_dir=checkpoint_dir, save_steps=800, save_secs=None)\n",
    "\n",
    "    summary_saver = tf.train.SummarySaverHook(save_steps=1,\n",
    "                                              save_secs=None,\n",
    "                                              output_dir=summary_dir, \n",
    "                                              summary_op=[sum_op_dis, sum_op_gen, sum_op_im]\n",
    "                                             )\n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    \n",
    "    mnt = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=[step_saver, summary_saver])\n",
    "\n",
    "                                   \n",
    "\n",
    "    with mnt as sess:\n",
    "\n",
    "        train_batch = 0\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "        while not mnt.should_stop() and epoch < nb_epochs:\n",
    "\n",
    "            begin = time.time()\n",
    "            nr_batches_train = int(trainx.shape[0] / batch_size)\n",
    "\n",
    "            # shuffling dataset\n",
    "            trainx = shuffle(trainx)  \n",
    "            train_loss_dis, train_loss_gen, train_loss_enc = [0, 0, 0]\n",
    "\n",
    "            # training\n",
    "            for t in range(nr_batches_train):\n",
    "\n",
    "                print(\"Starting Epoch {}, Batch {}, Step {}\".format(epoch+1, t+1, step+1))     \n",
    "                ran_from = t * batch_size\n",
    "                ran_to = (t + 1) * batch_size\n",
    "\n",
    "                # train discriminator\n",
    "                feed_dict = {x: trainx[ran_from:ran_to],\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "\n",
    "                _, ld, sm = sess.run([dis_op,\n",
    "                                      loss_discriminator,\n",
    "                                      sum_op_dis],\n",
    "                                     feed_dict=feed_dict)\n",
    "                train_loss_dis += ld\n",
    "\n",
    "                # train generator and encoder\n",
    "                feed_dict = {x: trainx[ran_from:ran_to],\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "                _,_, le, lg, sm = sess.run([gen_op,\n",
    "                                            enc_op,\n",
    "                                            loss_encoder,\n",
    "                                            loss_generator,\n",
    "                                            sum_op_gen],\n",
    "                                           feed_dict=feed_dict)\n",
    "                train_loss_gen += lg\n",
    "                train_loss_enc += le\n",
    "\n",
    "                if t % FREQ_PRINT == 0:  # inspect reconstruction\n",
    "                    t= np.random.randint(0,trainx.shape[0]-batch_size)\n",
    "                    ran_from = t\n",
    "                    ran_to = t + 4\n",
    "                    sm = sess.run(sum_op_im, feed_dict={x: trainx[ran_from:ran_to],training_mode: False})\n",
    "\n",
    "                train_batch += 1\n",
    "                step+=1\n",
    "\n",
    "            train_loss_gen /= nr_batches_train\n",
    "            train_loss_enc /= nr_batches_train\n",
    "            train_loss_dis /= nr_batches_train\n",
    "\n",
    "            print(\"Epoch %d | time = %ds | loss gen = %.4f | loss enc = %.4f | loss dis = %.4f \"\n",
    "                  % (epoch+1, time.time() - begin, train_loss_gen, train_loss_enc, train_loss_dis))\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Test\n",
    "            \n",
    "            if (epoch+1) % freq_epoch_test == 0:\n",
    "                print(\"Evaluating\")\n",
    "                \n",
    "                # Shuffle\n",
    "#                 testx = shuffle(testx)\n",
    "#                 blowhole_x, blowhole_y = shuffle(blowhole_x, blowhole_y)\n",
    "#                 break_x, break_y = shuffle(break_x, break_y)\n",
    "#                 crack_x, crack_y = shuffle(crack_x, crack_y)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Free Test\n",
    "                free_score_summary, free_t_img_summary, current_step, mean_score, var_score = \\\n",
    "                                                                            sess.run([sum_op_scr, sum_op_t_img, gs,\n",
    "                                                                                     mean, var], \n",
    "                                                                            feed_dict={x: testx[0:num_test_images],\n",
    "                                                                                       gt: np.zeros_like(testx),\n",
    "                                                                                       mean_inp: 0,\n",
    "                                                                                       var_inp: 0,\n",
    "                                                                                       training_mode: False})\n",
    "                free_writer.add_summary(free_score_summary, current_step)\n",
    "                free_writer.add_summary(free_t_img_summary, current_step)\n",
    "                free_writer.flush()\n",
    "                \n",
    "\n",
    "                # Blowhole\n",
    "                blowhole_score_summary, blowhole_t_img_summary, blowhole_acc_summary = \\\n",
    "                                                                          sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                          feed_dict={x: blowhole_x[0:num_test_images],\n",
    "                                                                                     gt: blowhole_y[0:num_test_images],\n",
    "                                                                                     mean_inp: mean_score,\n",
    "                                                                                     var_inp: var_score,\n",
    "                                                                                     training_mode: False})\n",
    "                blowhole_writer.add_summary(blowhole_score_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_t_img_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_acc_summary, current_step)\n",
    "                blowhole_writer.flush()\n",
    "                \n",
    "                # Crack\n",
    "                crack_score_summary, crack_t_img_summary, crack_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: crack_x[0:num_test_images], \n",
    "                                                                               gt: crack_y[0:num_test_images],\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                crack_writer.add_summary(crack_score_summary, current_step)\n",
    "                crack_writer.add_summary(crack_t_img_summary, current_step)\n",
    "                crack_writer.add_summary(crack_acc_summary, current_step)\n",
    "                crack_writer.flush()\n",
    "                    \n",
    "                # Break\n",
    "                break_score_summary, break_t_img_summary, break_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: break_x[0:num_test_images],\n",
    "                                                                               gt: break_y[0:num_test_images],\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                break_writer.add_summary(break_score_summary, current_step)\n",
    "                break_writer.add_summary(break_t_img_summary, current_step)\n",
    "                break_writer.add_summary(break_acc_summary, current_step)\n",
    "                break_writer.flush()\n",
    "                \n",
    "            \n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (4, 200)\n",
      "Output shape of layer_01 is (4, 8, 8, 1024)\n",
      "Output shape of layer_02 is (4, 16, 16, 512)\n",
      "Output shape of layer_03 is (4, 32, 32, 256)\n",
      "Output shape of layer_04 is (4, 64, 64, 128)\n",
      "Output shape of layer_05 is (4, 128, 128, 64)\n",
      "Output shape of layer_06 is (4, 256, 256, 3)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (4, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (4, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (4, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (4, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (4, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (4, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (4, 200)\n",
      "Output shape of z_layer_01 is (4, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (4, 66560)\n",
      "Output shape of xz_layer_01 is (4, 1024)\n",
      "Output shape of xz_layer_02 is (4, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "INFO:tensorflow:Summary name mean_score_w=0.1 is illegal; using mean_score_w_0.1 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.2 is illegal; using mean_score_w_0.2 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.3 is illegal; using mean_score_w_0.3 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.4 is illegal; using mean_score_w_0.4 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.5 is illegal; using mean_score_w_0.5 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_0 is illegal; using threshold_with_w_0.1__stddev_0 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_1 is illegal; using threshold_with_w_0.1__stddev_1 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_2 is illegal; using threshold_with_w_0.1__stddev_2 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_3 is illegal; using threshold_with_w_0.1__stddev_3 instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from train/train01/model.ckpt-0\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/train01/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/train01/model.ckpt.\n",
      "Starting Epoch 1, Batch 1, Step 1\n",
      "Starting Epoch 1, Batch 2, Step 2\n",
      "Starting Epoch 1, Batch 3, Step 3\n",
      "Starting Epoch 1, Batch 4, Step 4\n",
      "Epoch 1 | time = 9s | loss gen = 0.8448 | loss enc = 0.5654 | loss dis = 2.0685 \n",
      "Starting Epoch 2, Batch 1, Step 5\n",
      "Starting Epoch 2, Batch 2, Step 6\n",
      "Starting Epoch 2, Batch 3, Step 7\n",
      "Starting Epoch 2, Batch 4, Step 8\n",
      "Epoch 2 | time = 2s | loss gen = 1.2636 | loss enc = 0.6867 | loss dis = 2.1419 \n",
      "Evaluating\n",
      "Starting Epoch 3, Batch 1, Step 9\n",
      "Starting Epoch 3, Batch 2, Step 10\n",
      "Starting Epoch 3, Batch 3, Step 11\n",
      "Starting Epoch 3, Batch 4, Step 12\n",
      "Epoch 3 | time = 2s | loss gen = 0.7383 | loss enc = 0.6850 | loss dis = 2.3962 \n",
      "Starting Epoch 4, Batch 1, Step 13\n",
      "Starting Epoch 4, Batch 2, Step 14\n",
      "Starting Epoch 4, Batch 3, Step 15\n",
      "Starting Epoch 4, Batch 4, Step 16\n",
      "Epoch 4 | time = 2s | loss gen = 0.7884 | loss enc = 0.5881 | loss dis = 2.0317 \n",
      "Evaluating\n",
      "Starting Epoch 5, Batch 1, Step 17\n",
      "Starting Epoch 5, Batch 2, Step 18\n",
      "Starting Epoch 5, Batch 3, Step 19\n",
      "Starting Epoch 5, Batch 4, Step 20\n",
      "Epoch 5 | time = 2s | loss gen = 1.0970 | loss enc = 0.6507 | loss dis = 1.7263 \n",
      "Starting Epoch 6, Batch 1, Step 21\n",
      "Starting Epoch 6, Batch 2, Step 22\n",
      "Starting Epoch 6, Batch 3, Step 23\n",
      "Starting Epoch 6, Batch 4, Step 24\n",
      "Epoch 6 | time = 2s | loss gen = 0.8695 | loss enc = 0.5642 | loss dis = 1.9481 \n",
      "Evaluating\n",
      "Starting Epoch 7, Batch 1, Step 25\n",
      "Starting Epoch 7, Batch 2, Step 26\n",
      "Starting Epoch 7, Batch 3, Step 27\n",
      "Starting Epoch 7, Batch 4, Step 28\n",
      "Epoch 7 | time = 2s | loss gen = 0.8854 | loss enc = 0.5490 | loss dis = 2.0464 \n",
      "Starting Epoch 8, Batch 1, Step 29\n",
      "Starting Epoch 8, Batch 2, Step 30\n",
      "Starting Epoch 8, Batch 3, Step 31\n",
      "Starting Epoch 8, Batch 4, Step 32\n",
      "Epoch 8 | time = 2s | loss gen = 0.9989 | loss enc = 0.5925 | loss dis = 1.9151 \n",
      "Evaluating\n",
      "Starting Epoch 9, Batch 1, Step 33\n",
      "Starting Epoch 9, Batch 2, Step 34\n",
      "Starting Epoch 9, Batch 3, Step 35\n",
      "Starting Epoch 9, Batch 4, Step 36\n",
      "Epoch 9 | time = 2s | loss gen = 0.8281 | loss enc = 0.6250 | loss dis = 1.6060 \n",
      "Starting Epoch 10, Batch 1, Step 37\n",
      "Starting Epoch 10, Batch 2, Step 38\n",
      "Starting Epoch 10, Batch 3, Step 39\n",
      "Starting Epoch 10, Batch 4, Step 40\n",
      "Epoch 10 | time = 2s | loss gen = 0.9853 | loss enc = 0.7047 | loss dis = 1.9541 \n",
      "Evaluating\n",
      "Starting Epoch 11, Batch 1, Step 41\n",
      "Starting Epoch 11, Batch 2, Step 42\n",
      "Starting Epoch 11, Batch 3, Step 43\n",
      "Starting Epoch 11, Batch 4, Step 44\n",
      "Epoch 11 | time = 2s | loss gen = 0.8751 | loss enc = 0.5893 | loss dis = 1.6616 \n",
      "Starting Epoch 12, Batch 1, Step 45\n",
      "Starting Epoch 12, Batch 2, Step 46\n",
      "Starting Epoch 12, Batch 3, Step 47\n",
      "Starting Epoch 12, Batch 4, Step 48\n",
      "Epoch 12 | time = 2s | loss gen = 0.7822 | loss enc = 0.6667 | loss dis = 1.9309 \n",
      "Evaluating\n",
      "Starting Epoch 13, Batch 1, Step 49\n",
      "Starting Epoch 13, Batch 2, Step 50\n",
      "Starting Epoch 13, Batch 3, Step 51\n",
      "Starting Epoch 13, Batch 4, Step 52\n",
      "Epoch 13 | time = 2s | loss gen = 0.8945 | loss enc = 0.6179 | loss dis = 1.7713 \n",
      "Starting Epoch 14, Batch 1, Step 53\n",
      "Starting Epoch 14, Batch 2, Step 54\n",
      "Starting Epoch 14, Batch 3, Step 55\n",
      "Starting Epoch 14, Batch 4, Step 56\n",
      "Epoch 14 | time = 2s | loss gen = 0.9352 | loss enc = 0.5413 | loss dis = 1.9343 \n",
      "Evaluating\n",
      "Starting Epoch 15, Batch 1, Step 57\n",
      "Starting Epoch 15, Batch 2, Step 58\n",
      "Starting Epoch 15, Batch 3, Step 59\n",
      "Starting Epoch 15, Batch 4, Step 60\n",
      "Epoch 15 | time = 2s | loss gen = 1.1258 | loss enc = 0.6629 | loss dis = 1.5311 \n",
      "Starting Epoch 16, Batch 1, Step 61\n",
      "Starting Epoch 16, Batch 2, Step 62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 16, Batch 3, Step 63\n",
      "Starting Epoch 16, Batch 4, Step 64\n",
      "Epoch 16 | time = 2s | loss gen = 1.0316 | loss enc = 0.7093 | loss dis = 1.9193 \n",
      "Evaluating\n",
      "Starting Epoch 17, Batch 1, Step 65\n",
      "Starting Epoch 17, Batch 2, Step 66\n",
      "Starting Epoch 17, Batch 3, Step 67\n",
      "Starting Epoch 17, Batch 4, Step 68\n",
      "Epoch 17 | time = 2s | loss gen = 0.8961 | loss enc = 0.5874 | loss dis = 1.9633 \n",
      "Starting Epoch 18, Batch 1, Step 69\n",
      "Starting Epoch 18, Batch 2, Step 70\n",
      "Starting Epoch 18, Batch 3, Step 71\n",
      "Starting Epoch 18, Batch 4, Step 72\n",
      "Epoch 18 | time = 2s | loss gen = 0.9101 | loss enc = 0.6380 | loss dis = 1.7799 \n",
      "Evaluating\n",
      "Starting Epoch 19, Batch 1, Step 73\n",
      "Starting Epoch 19, Batch 2, Step 74\n",
      "Starting Epoch 19, Batch 3, Step 75\n",
      "Starting Epoch 19, Batch 4, Step 76\n",
      "Epoch 19 | time = 2s | loss gen = 0.9441 | loss enc = 0.6173 | loss dis = 1.7431 \n",
      "Starting Epoch 20, Batch 1, Step 77\n",
      "Starting Epoch 20, Batch 2, Step 78\n",
      "Starting Epoch 20, Batch 3, Step 79\n",
      "Starting Epoch 20, Batch 4, Step 80\n",
      "Epoch 20 | time = 2s | loss gen = 0.9959 | loss enc = 0.5720 | loss dis = 1.4947 \n",
      "Evaluating\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ae8a4b13d8cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     train(train_images, test_images, blowhole_images_set, blowhole_gt_images_set, crack_images_set, crack_gt_images_set,\n\u001b[0;32m----> 3\u001b[0;31m          break_images_set, break_gt_images_set)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-7efb9f2de0f3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(trainx, testx, blowhole_x, blowhole_y, crack_x, crack_y, break_x, break_y)\u001b[0m\n\u001b[1;32m    345\u001b[0m                                                                                        \u001b[0mmean_inp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                                                                                        \u001b[0mvar_inp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                                                                                        training_mode: False})\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0mfree_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfree_score_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mfree_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfree_t_img_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    669\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1146\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1149\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1222\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1294\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1077\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m     \"\"\"\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    train(train_images, test_images, blowhole_images_set, blowhole_gt_images_set, crack_images_set, crack_gt_images_set,\n",
    "         break_images_set, break_gt_images_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
