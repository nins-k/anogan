{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_length(num, length):\n",
    "    diff_in_length = length - len(str(num))\n",
    "    return \"0\"*diff_in_length + str(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_and_gt_names(base_dir, sub_dir):\n",
    "    \n",
    "    img_names = sorted(glob.glob(os.path.join(base_dir, sub_dir, \"*.jpg\")))\n",
    "    gt_img_names = sorted(glob.glob(os.path.join(base_dir, sub_dir, \"*.png\")))\n",
    "    \n",
    "    # Check if names are in sequence\n",
    "    for i in range(len(img_names)):\n",
    "        assert img_names[i][:-3] == gt_img_names[i][:-3]\n",
    "        \n",
    "    return img_names, gt_img_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x/255)*2-1\n",
    "\n",
    "\n",
    "def denormalize(x):\n",
    "    return np.uint8((x+1)/2*255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save_with_gt(img_names, gt_img_names, save_dir, scale_size, random_crop_range,\n",
    "                          max_attempts_per_image, gt_thresh):\n",
    "    \n",
    "    os.makedirs(os.path.join(os.getcwd(), save_dir))\n",
    "\n",
    "    image_set = []\n",
    "    gt_image_set = []\n",
    "    \n",
    "    for i in range(len(gt_img_names)):\n",
    "        \n",
    "        image_gt = cv2.imread(gt_img_names[i])\n",
    "        image = cv2.imread(img_names[i])\n",
    "        \n",
    "        h,w,d = image_gt.shape\n",
    "        if w < h or w==h:\n",
    "            w_factor = scale_size/w\n",
    "            \n",
    "            img_gt_r = cv2.resize(image_gt, dsize=(0,0), fx=w_factor, fy=w_factor)\n",
    "            img_r = cv2.resize(image, dsize=(0,0), fx=w_factor, fy=w_factor)\n",
    "            \n",
    "            if(img_r.shape[0] == img_r.shape[1] == scale_size):\n",
    "                \n",
    "                gt_image_set.append(img_gt_r)\n",
    "                image_set.append(img_r)\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            h_range = img_r.shape[0] - scale_size\n",
    "            gt_score = len(np.nonzero(img_gt_r)[0])\n",
    "            image_thresh = (h_range // random_crop_range)+1\n",
    "            images_created = 0\n",
    "            \n",
    "            for m in range(max_attempts_per_image):\n",
    "                \n",
    "                hp = np.random.randint(low=0, high=h_range)\n",
    "                cropped_gt_img = img_gt_r[hp:hp+scale_size,:,:]\n",
    "                cropped_gt_score = len(np.nonzero(cropped_gt_img)[0])\n",
    "#                 print(\"Height is {}, Width is {}, Resized shape is {}, h_range is {},\\\n",
    "#                 hp is {}, cropped shape is {}\".format(h,w,img_r.shape,h_range,hp,cropped_gt_img.shape))\n",
    "                if cropped_gt_score >= gt_score*gt_thresh:\n",
    "                    \n",
    "                    cropped_img = img_r[hp:hp+scale_size,:,:]\n",
    "                    \n",
    "                    gt_image_set.append(cropped_gt_img)\n",
    "                    image_set.append(cropped_img)\n",
    "                    \n",
    "                    images_created += 1\n",
    "                    if images_created == image_thresh:\n",
    "                        break\n",
    "\n",
    "        else:\n",
    "            h_factor = scale_size/h\n",
    "            \n",
    "            img_gt_r = cv2.resize(image_gt, dsize=(0,0), fx=h_factor, fy=h_factor)\n",
    "            img_r = cv2.resize(image, dsize=(0,0), fx=h_factor, fy=h_factor)\n",
    "            \n",
    "            if(img_r.shape[0] == img_r.shape[1] == scale_size):\n",
    "                \n",
    "                gt_image_set.append(img_gt_r)\n",
    "                image_set.append(img_r)\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            w_range = img_r.shape[1] - scale_size\n",
    "            gt_score = len(np.nonzero(img_gt_r)[0])\n",
    "            image_thresh = (w_range // random_crop_range)+1\n",
    "            images_created = 0\n",
    "            \n",
    "            for m in range(max_attempts_per_image):\n",
    "                \n",
    "                wp = np.random.randint(low=0, high=w_range)\n",
    "                cropped_gt_img = img_gt_r[:,wp:wp+scale_size,:]\n",
    "#                 print(\"Height is {}, Width is {}, Resized shape is {}, w_range is {},\\\n",
    "#                 wp is {}, cropped shape is {}\".format(h,w,img_r.shape,w_range,wp,cropped_gt_img.shape))\n",
    "                cropped_gt_score = len(np.nonzero(cropped_gt_img)[0])\n",
    "                \n",
    "                if cropped_gt_score >= gt_score*gt_thresh:\n",
    "                    \n",
    "                    cropped_img = img_r[:,wp:wp+scale_size,:]\n",
    "                    \n",
    "                    gt_image_set.append(cropped_gt_img)\n",
    "                    image_set.append(cropped_img)\n",
    "                    \n",
    "                    images_created += 1\n",
    "                    if images_created == image_thresh:\n",
    "                        break\n",
    "    \n",
    "    for counter in range(len(image_set)):\n",
    "        cv2.imwrite(os.path.join(save_dir, \"{}.{}\".format(adjust_length(counter, length=4), \"jpg\")), image_set[counter])\n",
    "        cv2.imwrite(os.path.join(save_dir, \"{}.{}\".format(adjust_length(counter, length=4), \"png\")), gt_image_set[counter])\n",
    "        \n",
    "    return len(img_names), len(image_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scale_and_random_crop(file_names, scale_size=256, random_crop_range=50, image_count=None):\n",
    "    \n",
    "    '''\n",
    "    Scales the image such that the smaller side is equal to scale_size\n",
    "    Number of random patches extracted along the longer side = 1 + ((Longer Side Length - scale_size)//random_crop_range)\n",
    "    '''\n",
    "    \n",
    "    image_set = []\n",
    "    \n",
    "    if image_count is None:\n",
    "        image_count = len(file_names)\n",
    "        \n",
    "    for name in file_names[:image_count]:\n",
    "        image = cv2.imread(name)\n",
    "        h,w,d = image.shape\n",
    "        if w < h or w==h:\n",
    "            w_factor = scale_size/w\n",
    "            img_r = cv2.resize(image, dsize=(0,0), fx=w_factor, fy=w_factor)\n",
    "            h_range = img_r.shape[0] - scale_size\n",
    "            if(img_r.shape[0] == img_r.shape[1] == scale_size):\n",
    "                image_set.append(img_r)\n",
    "                continue\n",
    "\n",
    "            for j in range((h_range // random_crop_range)+1):\n",
    "                hp = np.random.randint(low=0, high=h_range)\n",
    "                rescaled_img = img_r[hp:hp+scale_size,:,:]\n",
    "                image_set.append(rescaled_img)\n",
    "\n",
    "        else:\n",
    "            h_factor = scale_size/h\n",
    "            img_r = cv2.resize(image, dsize=(0,0), fx=h_factor, fy=h_factor)\n",
    "            w_range = img_r.shape[1] - scale_size\n",
    "            if(img_r.shape[0] == img_r.shape[1] == scale_size):\n",
    "                image_set.append(img_r)\n",
    "                continue\n",
    "\n",
    "            for j in range((w_range // random_crop_range)+1):\n",
    "                wp = np.random.randint(low=0, high=w_range)\n",
    "                rescaled_img = img_r[:,wp:wp+scale_size,:]\n",
    "                image_set.append(rescaled_img)\n",
    "        \n",
    "    return normalize(np.asarray(image_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_normalize(image_names):\n",
    "    return normalize(np.asarray([cv2.imread(name) for name in image_names]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths for unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'datasets/magnetic_tiles'\n",
    "\n",
    "free_images_dir = 'MT_Free/Imgs'\n",
    "blowhole_images_dir = 'MT_Blowhole/Imgs'\n",
    "break_images_dir = 'MT_Break/Imgs'\n",
    "crack_images_dir = 'MT_Crack/Imgs'\n",
    "\n",
    "new_dataset_dir = 'datasets/rescaled_magnetic_tiles'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get names of unprocessed images and their gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_img_names, free_img_gt_names = get_img_and_gt_names(dataset_dir, free_images_dir)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(dataset_dir, crack_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and save defect images and their gt (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 57, Created 217, Conversion Rate 3.81\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(crack_img_names, \n",
    "                                crack_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, crack_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 115, Created 429, Conversion Rate 3.73\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(blowhole_img_names, \n",
    "                                blowhole_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, blowhole_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 85, Created 286, Conversion Rate 3.36\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(break_img_names, \n",
    "                                break_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, break_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and load free images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 952, Created 2960, Conversion Rate 3.11\n"
     ]
    }
   ],
   "source": [
    "free_images_set = scale_and_random_crop(free_img_names, scale_size=256, random_crop_range=50, image_count=None)\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(len(free_img_names), len(free_images_set),\n",
    "                                                                   len(free_images_set)/len(free_img_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Training Images: 2560, Free Test Images: 400\n"
     ]
    }
   ],
   "source": [
    "train_images, test_images = train_test_split(free_images_set, train_size=2560)\n",
    "print(\"Free Training Images: {}, Free Test Images: {}\".format(len(train_images), len(test_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load defect image names and their gt image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed defect images (256x256)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(new_dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(new_dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(new_dataset_dir, crack_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load defect images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "blowhole_images_set = load_and_normalize(blowhole_img_names)\n",
    "blowhole_gt_images_set = load_and_normalize(blowhole_img_gt_names)\n",
    "\n",
    "break_images_set = load_and_normalize(break_img_names)\n",
    "break_gt_images_set = load_and_normalize(break_img_gt_names)\n",
    "\n",
    "crack_images_set = load_and_normalize(crack_img_names)\n",
    "crack_gt_images_set = load_and_normalize(crack_img_gt_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Images: 2960\n",
      "Blowhole Images: 429\n",
      "Break Images: 286\n",
      "Crack Images: 217\n"
     ]
    }
   ],
   "source": [
    "print(\"Free Images: {}\\nBlowhole Images: {}\\nBreak Images: {}\\nCrack Images: {}\".format(\n",
    "                                                                                          len(free_images_set),\n",
    "                                                                                          len(blowhole_images_set),\n",
    "                                                                                          len(break_images_set),\n",
    "                                                                                          len(crack_images_set)\n",
    "                                                                                         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blowhole_test_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c5f9985d25df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mblowhole_test_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblowhole_test_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mblowhole_gt_test_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblowhole_gt_test_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mbreak_test_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbreak_test_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blowhole_test_images' is not defined"
     ]
    }
   ],
   "source": [
    "# Shuffle \n",
    "\n",
    "test_images = shuffle(test_images)\n",
    "blowhole_images_set, blowhole_gt_images_set = shuffle(blowhole_images_set, blowhole_gt_images_set)\n",
    "break_images_set, break_gt_images_set = shuffle(break_images_set, break_gt_images_set)\n",
    "crack_images_set, crack_gt_images_set = shuffle(crack_images_set, crack_gt_images_set)\n",
    "\n",
    "# # Retain 100 of each\n",
    "\n",
    "# test_images = test_images\n",
    "# blowhole_test_images = blowhole_test_images[:200]\n",
    "# blowhole_gt_test_images = blowhole_gt_test_images[:200]\n",
    "# break_test_images = break_test_images[:200]\n",
    "# break_gt_test_images = break_gt_test_images[:200]\n",
    "# crack_test_images = crack_test_images[:200]\n",
    "# crack_gt_test_images = crack_gt_test_images[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Training Images: 2560\n",
      "Free Test Images: 400\n",
      "Blowhole Test Images: 429\n",
      "Break Test Images: 286\n",
      "Crack Test Images: 217\n"
     ]
    }
   ],
   "source": [
    "print(\"Free Training Images: {}\\nFree Test Images: {}\\nBlowhole Test Images: {}\\\n",
    "\\nBreak Test Images: {}\\nCrack Test Images: {}\".format(\n",
    "                                                                                          len(train_images),\n",
    "                                                                                          len(test_images),\n",
    "                                                                                          len(blowhole_images_set),\n",
    "                                                                                          len(break_images_set),\n",
    "                                                                                          len(crack_images_set)\n",
    "                                                                                         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_out(i, k, s, p=None):\n",
    "    if p is None:\n",
    "        p = int((k-1)/2)\n",
    "    o = int((i-k + 2*p)/s + 1)\n",
    "    print(o, p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_model(z, reuse=False, training_mode=False):\n",
    "    \n",
    "    print(\"\\nGenerator:\\n\")\n",
    "    print(\"Input shape of z is {}\".format(z.shape))\n",
    "    \n",
    "    dcgan_kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.002)\n",
    "\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        \n",
    "        # inp.shape (100)\n",
    "        # out.shape (8*8*1024)\n",
    "        name='layer_01'\n",
    "        with tf.variable_scope(name): \n",
    "            z = tf.layers.dense(inputs=z, units=8*8*1024, kernel_initializer=dcgan_kernel_initializer)\n",
    "            # reshape to (batch_size,8,8,1024)\n",
    "            z = tf.reshape(z, (-1,8,8,1024))\n",
    "            z = tf.layers.batch_normalization(inputs=z)\n",
    "            z = tf.nn.leaky_relu(features=z, alpha=0.2)\n",
    "            print(\"Output shape of {} is {}\".format(name, z.shape))\n",
    "            \n",
    "        \n",
    "        # in.shape (8,8,1024)\n",
    "        # out.shape (16,16,512)\n",
    "        name='layer_02'\n",
    "        with tf.variable_scope(name):\n",
    "            z = tf.layers.conv2d_transpose(inputs=z,\n",
    "                                           filters=512,\n",
    "                                           kernel_size=(5,5),\n",
    "                                           strides=(2,2),\n",
    "                                           padding='SAME',\n",
    "                                           kernel_initializer = dcgan_kernel_initializer\n",
    "                                                )\n",
    "            z = tf.layers.batch_normalization(inputs=z, training=training_mode)\n",
    "            z = tf.nn.leaky_relu(features=z, alpha=0.2)\n",
    "            print(\"Output shape of {} is {}\".format(name, z.shape))\n",
    "                                 \n",
    "        \n",
    "        # in.shape (16,16,512)\n",
    "        # out.shape (32,32,256)\n",
    "        name='layer_03'\n",
    "        with tf.variable_scope(name):\n",
    "            z = tf.layers.conv2d_transpose(inputs=z,\n",
    "                                           filters=256,\n",
    "                                           kernel_size=(5,5),\n",
    "                                           strides=(2,2),\n",
    "                                           padding='SAME',\n",
    "                                           kernel_initializer = dcgan_kernel_initializer\n",
    "                                                )\n",
    "            z = tf.layers.batch_normalization(inputs=z, training=training_mode)\n",
    "            z = tf.nn.leaky_relu(features=z, alpha=0.2)\n",
    "            print(\"Output shape of {} is {}\".format(name, z.shape))\n",
    "                                 \n",
    "            \n",
    "        # in.shape (32,32,256)\n",
    "        # out.shape (64,64,128)\n",
    "        name='layer_04'\n",
    "        with tf.variable_scope(name):\n",
    "            z = tf.layers.conv2d_transpose(inputs=z,\n",
    "                                           filters=128,\n",
    "                                           kernel_size=(5,5),\n",
    "                                           strides=(2,2),\n",
    "                                           padding='SAME',\n",
    "                                           kernel_initializer = dcgan_kernel_initializer\n",
    "                                                )\n",
    "            z = tf.layers.batch_normalization(inputs=z, training=training_mode)\n",
    "            z = tf.nn.leaky_relu(features=z, alpha=0.2)\n",
    "            print(\"Output shape of {} is {}\".format(name, z.shape))\n",
    "                                 \n",
    "                                 \n",
    "        # in.shape (64,64,128)\n",
    "        # out.shape (128,128,64)\n",
    "        name='layer_05'\n",
    "        with tf.variable_scope(name):\n",
    "            z = tf.layers.conv2d_transpose(inputs=z,\n",
    "                                           filters=64,\n",
    "                                           kernel_size=(5,5),\n",
    "                                           strides=(2,2),\n",
    "                                           padding='SAME',\n",
    "                                           kernel_initializer = dcgan_kernel_initializer\n",
    "                                                )\n",
    "            z = tf.layers.batch_normalization(inputs=z, training=training_mode)\n",
    "            z = tf.nn.leaky_relu(features=z, alpha=0.2)\n",
    "            print(\"Output shape of {} is {}\".format(name, z.shape))\n",
    "                                 \n",
    "                        \n",
    "        # in.shape (128,128,64)\n",
    "        # out.shape (256,256,3)\n",
    "        name='layer_06'\n",
    "        with tf.variable_scope(name):\n",
    "            z = tf.layers.conv2d_transpose(inputs=z,\n",
    "                                           filters=3,\n",
    "                                           kernel_size=(5,5),\n",
    "                                           strides=(2,2),\n",
    "                                           padding='SAME',\n",
    "                                           kernel_initializer = dcgan_kernel_initializer\n",
    "                                                )\n",
    "            z = tf.tanh(z)\n",
    "            print(\"Output shape of {} is {}\".format(name, z.shape))\n",
    "        \n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_model(x, z, reuse=False, training_mode=False, penultimate_layer_units=1024):\n",
    "    \n",
    "    '''\n",
    "    For Real Images:\n",
    "    Input: x, E(x) \n",
    "    Output: Probability of image being real\n",
    "    \n",
    "    For Generated Images:\n",
    "    Input: G(z), z \n",
    "    Output: Probability of real image\n",
    "    '''\n",
    "    \n",
    "    print(\"\\nDiscriminator: \\n\")\n",
    "    print(\"Input shape of x is {}\".format(x.shape))\n",
    "    \n",
    "    dcgan_kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.002)\n",
    "\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "\n",
    "        # inp.shape (256,256,3)\n",
    "        # out.shape (128,128,64)\n",
    "        name='x_layer_01'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.conv2d(inputs=x, \n",
    "                                 filters=64, \n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='SAME', \n",
    "                                 kernel_initializer = dcgan_kernel_initializer\n",
    "                             )\n",
    "            x = tf.nn.leaky_relu(features=x, alpha=0.2)\n",
    "            x = tf.layers.dropout(inputs=x, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "            \n",
    "\n",
    "        # inp.shape (128,128,64)\n",
    "        # out.shape (64,64,128)\n",
    "        name='x_layer_02'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.conv2d(inputs=x, \n",
    "                                 filters=128, \n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='SAME', \n",
    "                                 kernel_initializer = dcgan_kernel_initializer\n",
    "                             )\n",
    "            x = tf.layers.batch_normalization(inputs=x, training=training_mode)\n",
    "            x = tf.nn.leaky_relu(features=x, alpha=0.2)\n",
    "            x = tf.layers.dropout(inputs=x, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "            \n",
    "\n",
    "        # inp.shape (64,64,128)\n",
    "        # out.shape (32,32,256)\n",
    "        name='x_layer_03'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.conv2d(inputs=x, \n",
    "                                 filters=256, \n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='SAME', \n",
    "                                 kernel_initializer = dcgan_kernel_initializer\n",
    "                             )\n",
    "            x = tf.layers.batch_normalization(inputs=x, training=training_mode)\n",
    "            x = tf.nn.leaky_relu(features=x, alpha=0.2)\n",
    "            x = tf.layers.dropout(inputs=x, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "\n",
    "\n",
    "        # inp.shape (32,32,256)\n",
    "        # out.shape (16,16,512)\n",
    "        name='x_layer_04'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.conv2d(inputs=x, \n",
    "                                 filters=512, \n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='SAME', \n",
    "                                 kernel_initializer = dcgan_kernel_initializer\n",
    "                             )\n",
    "            x = tf.layers.batch_normalization(inputs=x, training=training_mode)\n",
    "            x = tf.nn.leaky_relu(features=x, alpha=0.2)\n",
    "            x = tf.layers.dropout(inputs=x, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "\n",
    "\n",
    "        # inp.shape (16,16,512)\n",
    "        # out.shape (8,8,1024)\n",
    "        name='x_layer_05'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.conv2d(inputs=x, \n",
    "                                 filters=1024, \n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='SAME', \n",
    "                                 kernel_initializer=dcgan_kernel_initializer\n",
    "                             )\n",
    "            x = tf.layers.batch_normalization(inputs=x, training=training_mode)\n",
    "            x = tf.nn.leaky_relu(features=x, alpha=0.2)\n",
    "            x = tf.layers.dropout(inputs=x, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "\n",
    "        x = tf.reshape(x, (-1, 8*8*1024))\n",
    "        \n",
    "        \n",
    "        print(\"\\nInput shape of z is {}\".format(z.shape))\n",
    "        \n",
    "        # inp.shape (200)\n",
    "        # out.shape (1024)\n",
    "        name='z_layer_01'\n",
    "        with tf.variable_scope(name):\n",
    "            z = tf.layers.dense(inputs=z,\n",
    "                                units=1024,\n",
    "                                kernel_initializer=dcgan_kernel_initializer\n",
    "                               )\n",
    "            z = tf.layers.batch_normalization(inputs=z, training=training_mode)\n",
    "            z = tf.nn.leaky_relu(features=z, alpha=0.2)\n",
    "            z = tf.layers.dropout(inputs=z, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, z.shape))\n",
    "            \n",
    "        \n",
    "    \n",
    "        # z inp.shape (1024)\n",
    "        # x inp.shape (8*8*1024)\n",
    "        # concat[x,z] out.shape ()\n",
    "        xz = tf.concat([x,z], axis=1)\n",
    "        print(\"\\nOutput shape of [x,z] concat is {}\".format(xz.shape))\n",
    "        \n",
    "        \n",
    "        # inp.shape (66560)\n",
    "        # out.shape (penultimate_layer=1024)\n",
    "        name='xz_layer_01'\n",
    "        with tf.variable_scope(name):\n",
    "            xz = tf.layers.dense(inputs=xz,\n",
    "                                units=penultimate_layer_units,\n",
    "                                kernel_initializer=dcgan_kernel_initializer\n",
    "                               )\n",
    "            xz = tf.layers.batch_normalization(inputs=xz, training=training_mode)\n",
    "            xz = tf.nn.leaky_relu(features=xz, alpha=0.2)\n",
    "            xz = tf.layers.dropout(inputs=xz, rate=0.5, training=training_mode)\n",
    "        print(\"Output shape of {} is {}\".format(name, xz.shape))\n",
    "        \n",
    "        penultimate_layer = xz\n",
    "        \n",
    "        \n",
    "        # inp.shape (penultimate_layer=1024)\n",
    "        # out.shape (1)\n",
    "        name='xz_layer_02'\n",
    "        with tf.variable_scope(name):\n",
    "            xz = tf.layers.dense(inputs=xz,\n",
    "                                units=1,\n",
    "                                kernel_initializer=dcgan_kernel_initializer\n",
    "                               )\n",
    "            xz = tf.layers.batch_normalization(inputs=xz, training=training_mode)\n",
    "            xz = tf.nn.leaky_relu(features=xz, alpha=0.2)\n",
    "            xz = tf.layers.dropout(inputs=xz, rate=0.5, training=training_mode)\n",
    "        print(\"Output shape of {} is {}\".format(name, xz.shape))\n",
    "        \n",
    "        \n",
    "        return xz, penultimate_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_model(x, latent_dimensions, reuse=False, training_mode=False):\n",
    "    \n",
    "    '''\n",
    "    For Real Images:\n",
    "    Input: x \n",
    "    Output: E(x)\n",
    "    \n",
    "    For Generated Images:\n",
    "    Input: G(z) \n",
    "    Output: E(G(z))\n",
    "    '''\n",
    "    \n",
    "    print(\"\\nEncoder: \\n\")\n",
    "    print(\"Input shape of x is {}\".format(x.shape))\n",
    "    \n",
    "    dcgan_kernel_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.002)\n",
    "\n",
    "    with tf.variable_scope('Encoder', reuse=reuse):\n",
    "        \n",
    "        # inp.shape (256,256,3)\n",
    "        # out.shape (128,128,16)\n",
    "        name='layer_01'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.conv2d(inputs=x, \n",
    "                                 filters=16, \n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='SAME', \n",
    "                                 kernel_initializer = dcgan_kernel_initializer\n",
    "                             )\n",
    "            x = tf.layers.batch_normalization(inputs=x, training=training_mode)\n",
    "            x = tf.nn.leaky_relu(features=x, alpha=0.2)\n",
    "            x = tf.layers.dropout(inputs=x, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "            \n",
    "            \n",
    "        # inp.shape (128,128,16)\n",
    "        # out.shape (64,64,32)\n",
    "        name='layer_02'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.conv2d(inputs=x, \n",
    "                                 filters=32, \n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='SAME', \n",
    "                                 kernel_initializer = dcgan_kernel_initializer\n",
    "                             )\n",
    "            x = tf.layers.batch_normalization(inputs=x, training=training_mode)\n",
    "            x = tf.nn.leaky_relu(features=x, alpha=0.2)\n",
    "            x = tf.layers.dropout(inputs=x, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "            \n",
    "            \n",
    "        # inp.shape (64,64,32)\n",
    "        # out.shape (32,32,64)\n",
    "        name='layer_03'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.conv2d(inputs=x, \n",
    "                                 filters=64, \n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='SAME', \n",
    "                                 kernel_initializer = dcgan_kernel_initializer\n",
    "                             )\n",
    "            x = tf.layers.batch_normalization(inputs=x, training=training_mode)\n",
    "            x = tf.nn.leaky_relu(features=x, alpha=0.2)\n",
    "            x = tf.layers.dropout(inputs=x, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "            \n",
    "        \n",
    "        # inp.shape (32,32,64)\n",
    "        # out.shape (16,16,128)\n",
    "        name='layer_04'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.conv2d(inputs=x, \n",
    "                                 filters=128, \n",
    "                                 kernel_size=(5,5), \n",
    "                                 strides=(2,2), \n",
    "                                 padding='SAME', \n",
    "                                 kernel_initializer = dcgan_kernel_initializer\n",
    "                             )\n",
    "            x = tf.layers.batch_normalization(inputs=x, training=training_mode)\n",
    "            x = tf.nn.leaky_relu(features=x, alpha=0.2)\n",
    "            x = tf.layers.dropout(inputs=x, rate=0.5, training=training_mode)\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "            \n",
    "        \n",
    "        # inp.shape (16,16,128)\n",
    "        # out.shape (latent dimesnions)\n",
    "        name='layer_04'\n",
    "        with tf.variable_scope(name):     \n",
    "            x = tf.layers.flatten(x)\n",
    "            x = tf.layers.dense(inputs=x, \n",
    "                                units=latent_dimensions,\n",
    "                                kernel_initializer=dcgan_kernel_initializer\n",
    "                               )\n",
    "            print(\"Output shape of {} is {}\".format(name, x.shape))\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Test Model Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Test Generator\n",
    "placeholder = tf.placeholder(dtype=tf.float32, shape=(None,200))\n",
    "gen = get_generator_model(placeholder, tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n"
     ]
    }
   ],
   "source": [
    "# Test Discriminator\n",
    "placeholder = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "placeholder2 = tf.placeholder(dtype=tf.float32, shape=(None,200))\n",
    "dis = get_discriminator_model(placeholder, placeholder2,tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n"
     ]
    }
   ],
   "source": [
    "# Test Encoder\n",
    "placeholder = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "en = get_encoder_model(placeholder, latent_dimensions=200, reuse=tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_dir(checkpoint_dir):\n",
    "    r = glob.glob(os.path.join(checkpoint_dir, \"logs*\"))\n",
    "    log_dir_name = os.path.join(checkpoint_dir, \"logs{}\".format(str(len(r)+1)))\n",
    "                  \n",
    "    return log_dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainx, testx, blowhole_x, blowhole_y, crack_x, crack_y, break_x, break_y):\n",
    "    \n",
    "    trainx = trainx[:16]\n",
    "    penultimate_layer_units = 1024\n",
    "    latent_dimensions = 200\n",
    "    batch_size = 8\n",
    "    FREQ_PRINT = 80\n",
    "    learning_rate = 0.0002\n",
    "    nb_epochs = 500\n",
    "    freq_epoch_test = 2\n",
    "    num_test_images = 4\n",
    "    \n",
    "    # Image input placeholder\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # Ground truth input placeholder\n",
    "    gt = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # mean and variance of the free image scores\n",
    "    mean_inp = tf.placeholder(dtype=tf.float32)\n",
    "    var_inp = tf.placeholder(dtype=tf.float32)\n",
    "    \n",
    "    # Training mode placeholder\n",
    "    training_mode = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_real_image = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=False, training_mode=True)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        z = tf.random_normal([batch_size, latent_dimensions])\n",
    "        generated_image = get_generator_model(z, reuse=False, training_mode=True)\n",
    "        regenerated_real_image = get_generator_model(encoding_real_image, reuse=True, training_mode=False)\n",
    "    \n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake, dis_fake_penultimate_layer = get_discriminator_model(generated_image, z, reuse=False, \n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "        discriminator_real, dis_real_penultimate_layer = get_discriminator_model(x, encoding_real_image, reuse=True,\n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "    \n",
    "    # Prepare labels for the loss functions\n",
    "    with tf.variable_scope('labels'):\n",
    "        \n",
    "        # Step 1\n",
    "        # Set swapped labels\n",
    "        labels_dis_enc = tf.zeros_like(discriminator_real)\n",
    "        labels_dis_gen = tf.ones_like(discriminator_fake)\n",
    "        labels_gen = tf.zeros_like(discriminator_fake)\n",
    "        labels_enc = tf.ones_like(discriminator_real)\n",
    "        \n",
    "        # Step 2\n",
    "        # Create soft labels for the discriminator\n",
    "        random_soft = tf.random_uniform(shape=(tf.shape(labels_dis_enc)), minval=0.0, maxval=0.1)\n",
    "        soft_labels_dis_enc = tf.add(labels_dis_enc, random_soft)\n",
    "        soft_labels_dis_gen = tf.subtract(labels_dis_gen, random_soft)\n",
    "\n",
    "        # Step 3\n",
    "        # With a low chance, assign noisy (swapped) labels\n",
    "        random_flip = tf.ones_like(labels_dis_enc) * tf.random_uniform(shape=(1,), minval=0, maxval=1)\n",
    "        mask = random_flip >= 0.05\n",
    "        labels_dis_enc = tf.where(mask, soft_labels_dis_enc, soft_labels_dis_gen)\n",
    "        labels_dis_gen = tf.where(mask, soft_labels_dis_gen, soft_labels_dis_enc)\n",
    "    \n",
    "    # Loss Functions\n",
    "    with tf.variable_scope('loss_functions'):\n",
    "        loss_dis_enc = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_dis_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "        loss_dis_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(discriminator_fake),\n",
    "                                                                              logits=discriminator_fake))\n",
    "        loss_discriminator = loss_dis_gen + loss_dis_enc\n",
    "        # generator\n",
    "        loss_generator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_gen,\n",
    "                                                                                logits=discriminator_fake))\n",
    "        # encoder\n",
    "        loss_encoder = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "    with tf.name_scope('optimizers'):\n",
    "        # control op dependencies for batch norm and trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        dvars = [var for var in tvars if 'discriminator_model' in var.name]\n",
    "        gvars = [var for var in tvars if 'generator_model' in var.name]\n",
    "        evars = [var for var in tvars if 'encoder_model' in var.name]\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        update_ops_gen = [x for x in update_ops if ('generator_model' in x.name)]\n",
    "        update_ops_enc = [x for x in update_ops if ('encoder_model' in x.name)]\n",
    "        update_ops_dis = [x for x in update_ops if ('discriminator_model' in x.name)]\n",
    "\n",
    "        optimizer_dis = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='dis_optimizer')\n",
    "        optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='gen_optimizer')\n",
    "        optimizer_enc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='enc_optimizer')\n",
    "\n",
    "        with tf.control_dependencies(update_ops_gen):\n",
    "            gen_op = optimizer_gen.minimize(loss_generator, var_list=gvars)\n",
    "        with tf.control_dependencies(update_ops_enc):\n",
    "            enc_op = optimizer_enc.minimize(loss_encoder, var_list=evars, global_step=tf.train.get_or_create_global_step())\n",
    "        with tf.control_dependencies(update_ops_dis):\n",
    "            dis_op = optimizer_dis.minimize(loss_discriminator, var_list=dvars)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        with tf.name_scope('discriminator'):\n",
    "            tf.summary.scalar('loss_total', loss_discriminator, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_enc', loss_dis_enc, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_gen', loss_dis_gen, ['dis'])\n",
    "\n",
    "        with tf.name_scope('generator'):\n",
    "            tf.summary.scalar('loss_generator', loss_generator, ['gen'])\n",
    "            tf.summary.scalar('loss_encoder', loss_encoder, ['gen'])\n",
    "\n",
    "    with tf.name_scope('train_img_regen'):\n",
    "        for p in range(4):\n",
    "            tf.summary.image('img_{}_regen'.format(p+1), regenerated_real_image[p:p+1,:,:,:], 1, ['image'])\n",
    "            tf.summary.image('img_{}_input'.format(p+1), x[p:p+1,:,:,:], 1, ['image'])\n",
    "\n",
    "    sum_op_dis = tf.summary.merge_all('dis')\n",
    "    sum_op_gen = tf.summary.merge_all('gen')\n",
    "    sum_op_im = tf.summary.merge_all('image')\n",
    "\n",
    "        \n",
    "    '''\n",
    "    ----------------------------------------TRAINING OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "        \n",
    "    # TESTING GRAPH\n",
    "\n",
    "\n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_test = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=True, training_mode=False)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        regenerated_image_test = get_generator_model(encoding_test, reuse=True, training_mode=False)\n",
    "\n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake_test, dis_fake_penultimate_layer_test = get_discriminator_model(regenerated_image_test, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True, \n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "        discriminator_real_test, dis_real_penultimate_layer_test = get_discriminator_model(x, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True,\n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "    with tf.name_scope('testing'):\n",
    "        with tf.variable_scope('reconstruction_loss'):\n",
    "            delta = x - regenerated_image_test\n",
    "            delta_flat = tf.layers.flatten(delta)\n",
    "            gen_score = tf.norm(delta_flat, ord='euclidean', axis=1,\n",
    "                              keep_dims=False, name='epsilon')\n",
    "\n",
    "        with tf.variable_scope('discriminator_loss'):\n",
    "            fm = dis_real_penultimate_layer_test - dis_fake_penultimate_layer_test\n",
    "            fm = tf.contrib.layers.flatten(fm)\n",
    "            dis_score = tf.norm(fm, ord='euclidean', axis=1,\n",
    "                             keep_dims=False, name='d_loss')\n",
    "            dis_score = tf.squeeze(dis_score)\n",
    "\n",
    "            \n",
    "        weight1, weight2, weight3, weight4, weight5 = 0.1, 0.2, 0.3, 0.4, 0.5 \n",
    "        \n",
    "        with tf.variable_scope('score'):\n",
    "            mean_score1 = tf.reduce_mean((1 - weight1) * gen_score + weight1 * dis_score)\n",
    "            mean_score2 = tf.reduce_mean((1 - weight2) * gen_score + weight2 * dis_score)\n",
    "            mean_score3 = tf.reduce_mean((1 - weight3) * gen_score + weight3 * dis_score)\n",
    "            mean_score4 = tf.reduce_mean((1 - weight4) * gen_score + weight4 * dis_score)\n",
    "            mean_score5 = tf.reduce_mean((1 - weight5) * gen_score + weight5 * dis_score)\n",
    "            \n",
    "\n",
    "    with tf.name_scope('test_anomaly_score'):\n",
    "        tf.summary.scalar(\"mean_score_w=0.1\", mean_score1, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.2\", mean_score2, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.3\", mean_score3, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.4\", mean_score4, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.5\", mean_score5, ['scr'])\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('accuracy'):\n",
    "        \n",
    "        # For defect accuracy calculation\n",
    "        all_test_scores = (1 - weight1) * gen_score + weight1 * dis_score\n",
    "        free_thresh_0 = mean_inp\n",
    "        free_thresh_1 = mean_inp + tf.sqrt(var_inp)\n",
    "        free_thresh_2 = mean_inp + 2* tf.sqrt(var_inp)\n",
    "        free_thresh_3 = mean_inp + 3 * tf.sqrt(var_inp)\n",
    "        \n",
    "        bool_list_0 = tf.greater_equal(all_test_scores, free_thresh_0)\n",
    "        test_acc_0 = tf.reduce_sum(tf.cast(bool_list_0, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_1 = tf.greater_equal(all_test_scores, free_thresh_1)\n",
    "        test_acc_1 = tf.reduce_sum(tf.cast(bool_list_1, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_2 = tf.greater_equal(all_test_scores, free_thresh_2)\n",
    "        test_acc_2 = tf.reduce_sum(tf.cast(bool_list_2, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_3 = tf.greater_equal(all_test_scores, free_thresh_3)\n",
    "        test_acc_3 = tf.reduce_sum(tf.cast(bool_list_3, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        # For calculating optimal anomaly score based on free image scores\n",
    "        mean, var = tf.nn.moments(all_test_scores, axes=[0])\n",
    "        \n",
    "    \n",
    "    with tf.name_scope('test_accuracy'):\n",
    "        \n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_0', test_acc_0, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_1', test_acc_1, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_2', test_acc_2, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_3', test_acc_3, ['test_acc'])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('test_img_regen'):\n",
    "        for p in range(2):\n",
    "            tf.summary.image('{}_0_input'.format(p+1), x[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_1_regen'.format(p+1), regenerated_image_test[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_2_ground_truth'.format(p+1), gt[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_3_difference'.format(p+1), delta[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            \n",
    "            \n",
    "    sum_op_scr = tf.summary.merge_all('scr')\n",
    "    sum_op_t_img = tf.summary.merge_all('t_image')\n",
    "    sum_op_test_acc = tf.summary.merge_all('test_acc')\n",
    "    \n",
    "    gs = tf.train.get_global_step()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ----------------------------------------TEST OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    # TRAINING\n",
    "\n",
    "    checkpoint_dir = \"train/train01/\"\n",
    "    summary_dir = get_summary_dir(checkpoint_dir)\n",
    "    \n",
    "    free_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"free\"))\n",
    "    blowhole_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"blowhole\"))\n",
    "    crack_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"crack\"))\n",
    "    break_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"break\"))\n",
    "\n",
    "\n",
    "    step_saver =tf.train.CheckpointSaverHook(checkpoint_dir=checkpoint_dir, save_steps=800, save_secs=None)\n",
    "\n",
    "    summary_saver = tf.train.SummarySaverHook(save_steps=1,\n",
    "                                              save_secs=None,\n",
    "                                              output_dir=summary_dir, \n",
    "                                              summary_op=[sum_op_dis, sum_op_gen, sum_op_im]\n",
    "                                             )\n",
    "\n",
    "    mnt = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, hooks=[step_saver, summary_saver])\n",
    "\n",
    "                                   \n",
    "\n",
    "    with mnt as sess:\n",
    "\n",
    "        train_batch = 0\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "        while not mnt.should_stop() and epoch < nb_epochs:\n",
    "\n",
    "            begin = time.time()\n",
    "            nr_batches_train = int(trainx.shape[0] / batch_size)\n",
    "\n",
    "            # shuffling dataset\n",
    "            trainx = shuffle(trainx)  \n",
    "            train_loss_dis, train_loss_gen, train_loss_enc = [0, 0, 0]\n",
    "\n",
    "            # training\n",
    "            for t in range(nr_batches_train):\n",
    "\n",
    "                print(\"Starting Epoch {}, Batch {}, Step {}\".format(epoch+1, t+1, step+1))     \n",
    "                ran_from = t * batch_size\n",
    "                ran_to = (t + 1) * batch_size\n",
    "\n",
    "                # train discriminator\n",
    "                feed_dict = {x: trainx[ran_from:ran_to],\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "\n",
    "                _, ld, sm = sess.run([dis_op,\n",
    "                                      loss_discriminator,\n",
    "                                      sum_op_dis],\n",
    "                                     feed_dict=feed_dict)\n",
    "                train_loss_dis += ld\n",
    "\n",
    "                # train generator and encoder\n",
    "                feed_dict = {x: trainx[ran_from:ran_to],\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "                _,_, le, lg, sm = sess.run([gen_op,\n",
    "                                            enc_op,\n",
    "                                            loss_encoder,\n",
    "                                            loss_generator,\n",
    "                                            sum_op_gen],\n",
    "                                           feed_dict=feed_dict)\n",
    "                train_loss_gen += lg\n",
    "                train_loss_enc += le\n",
    "\n",
    "                if t % FREQ_PRINT == 0:  # inspect reconstruction\n",
    "                    t= np.random.randint(0,trainx.shape[0]-batch_size)\n",
    "                    ran_from = t\n",
    "                    ran_to = t + 4\n",
    "                    sm = sess.run(sum_op_im, feed_dict={x: trainx[ran_from:ran_to],training_mode: False})\n",
    "\n",
    "                train_batch += 1\n",
    "                step+=1\n",
    "\n",
    "            train_loss_gen /= nr_batches_train\n",
    "            train_loss_enc /= nr_batches_train\n",
    "            train_loss_dis /= nr_batches_train\n",
    "\n",
    "            print(\"Epoch %d | time = %ds | loss gen = %.4f | loss enc = %.4f | loss dis = %.4f \"\n",
    "                  % (epoch+1, time.time() - begin, train_loss_gen, train_loss_enc, train_loss_dis))\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Test\n",
    "            \n",
    "            if (epoch+1) % freq_epoch_test == 0:\n",
    "                print(\"Evaluating\")\n",
    "                \n",
    "                # Shuffle\n",
    "                testx = shuffle(testx)\n",
    "                blowhole_x, blowhole_y = shuffle(blowhole_x, blowhole_y)\n",
    "                break_x, break_y = shuffle(break_x, break_y)\n",
    "                crack_x, crack_y = shuffle(crack_x, crack_y)\n",
    "                \n",
    "                \n",
    "                \n",
    "                # Free Test\n",
    "                free_score_summary, free_t_img_summary, current_step, current_threshold, mean_score, var_score = \\\n",
    "                                                                            sess.run([sum_op_scr, sum_op_t_img, gs,\n",
    "                                                                                     mean, var], \n",
    "                                                                            feed_dict={x: testx[0:num_test_images],\n",
    "                                                                                       gt: np.zeros_like(testx),\n",
    "                                                                                       mean_inp: 0,\n",
    "                                                                                       var_inp: 0,\n",
    "                                                                                       training_mode: False})\n",
    "                free_writer.add_summary(free_score_summary, current_step)\n",
    "                free_writer.add_summary(free_t_img_summary, current_step)\n",
    "                free_writer.flush()\n",
    "                \n",
    "\n",
    "                # Blowhole\n",
    "                blowhole_score_summary, blowhole_t_img_summary, blowhole_acc_summary = \\\n",
    "                                                                          sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                          feed_dict={x: blowhole_x[0:num_test_images],\n",
    "                                                                                     gt: blowhole_y[0:num_test_images],\n",
    "                                                                                     mean_inp: mean_score,\n",
    "                                                                                     var_inp: var_score,\n",
    "                                                                                     training_mode: False})\n",
    "                blowhole_writer.add_summary(blowhole_score_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_t_img_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_acc_summary, current_step)\n",
    "                blowhole_writer.flush()\n",
    "                \n",
    "                # Crack\n",
    "                crack_score_summary, crack_t_img_summary, crack_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: crack_x[0:num_test_images], \n",
    "                                                                               gt: crack_y[0:num_test_images],\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                crack_writer.add_summary(crack_score_summary, current_step)\n",
    "                crack_writer.add_summary(crack_t_img_summary, current_step)\n",
    "                crack_writer.add_summary(crack_acc_summary, current_step)\n",
    "                crack_writer.flush()\n",
    "                    \n",
    "                # Break\n",
    "                break_score_summary, break_t_img_summary, break_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: break_x[0:num_test_images],\n",
    "                                                                               gt: break_y[0:num_test_images],\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                break_writer.add_summary(break_score_summary, current_step)\n",
    "                break_writer.add_summary(break_t_img_summary, current_step)\n",
    "                break_writer.add_summary(break_acc_summary, current_step)\n",
    "                break_writer.flush()\n",
    "                \n",
    "            \n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (8, 200)\n",
      "Output shape of layer_01 is (8, 8, 8, 1024)\n",
      "Output shape of layer_02 is (8, 16, 16, 512)\n",
      "Output shape of layer_03 is (8, 32, 32, 256)\n",
      "Output shape of layer_04 is (8, 64, 64, 128)\n",
      "Output shape of layer_05 is (8, 128, 128, 64)\n",
      "Output shape of layer_06 is (8, 256, 256, 3)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (8, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (8, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (8, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (8, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (8, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (8, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (8, 200)\n",
      "Output shape of z_layer_01 is (8, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (8, 66560)\n",
      "Output shape of xz_layer_01 is (8, 1024)\n",
      "Output shape of xz_layer_02 is (8, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "WARNING:tensorflow:From <ipython-input-22-484bc0869c5f>:158: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Summary name mean_score_w=0.1 is illegal; using mean_score_w_0.1 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.2 is illegal; using mean_score_w_0.2 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.3 is illegal; using mean_score_w_0.3 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.4 is illegal; using mean_score_w_0.4 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.5 is illegal; using mean_score_w_0.5 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_0 is illegal; using threshold_with_w_0.1__stddev_0 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_1 is illegal; using threshold_with_w_0.1__stddev_1 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_2 is illegal; using threshold_with_w_0.1__stddev_2 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_3 is illegal; using threshold_with_w_0.1__stddev_3 instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/train01/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/train01/model.ckpt.\n",
      "Starting Epoch 1, Batch 1, Step 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    train(train_images, test_images, blowhole_images_set, blowhole_gt_images_set, crack_images_set, crack_gt_images_set,\n",
    "         break_images_set, break_gt_images_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
