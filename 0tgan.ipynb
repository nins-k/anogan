{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_components import *\n",
    "from data_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import cv2\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set paths for unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'datasets/magnetic_tiles'\n",
    "\n",
    "free_images_dir = 'MT_Free/Imgs'\n",
    "blowhole_images_dir = 'MT_Blowhole/Imgs'\n",
    "break_images_dir = 'MT_Break/Imgs'\n",
    "crack_images_dir = 'MT_Crack/Imgs'\n",
    "\n",
    "new_dataset_dir = 'datasets/rescaled_magnetic_tiles'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get names of unprocessed images and their gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "free_img_names, free_img_gt_names = get_img_and_gt_names(dataset_dir, free_images_dir)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(dataset_dir, crack_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and save defect images and their gt (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 57, Created 214, Conversion Rate 3.75\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(crack_img_names, \n",
    "                                crack_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, crack_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 115, Created 429, Conversion Rate 3.73\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(blowhole_img_names, \n",
    "                                blowhole_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, blowhole_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 85, Created 283, Conversion Rate 3.33\n"
     ]
    }
   ],
   "source": [
    "inp, op = crop_and_save_with_gt(break_img_names, \n",
    "                                break_img_gt_names, \n",
    "                                save_dir=os.path.join(new_dataset_dir, break_images_dir),\n",
    "                                scale_size=256,\n",
    "                                random_crop_range=50,\n",
    "                                max_attempts_per_image=100,\n",
    "                                gt_thresh=0.5\n",
    "                               )\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op, op/inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process and save free images (256x256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Names: 952, Created 2960, Conversion Rate 3.11\n"
     ]
    }
   ],
   "source": [
    "inp, op = scale_and_random_crop(free_img_names, \n",
    "                                scale_size=256, \n",
    "                                random_crop_range=50, \n",
    "                                image_count=None, \n",
    "                                save_dir=os.path.join(new_dataset_dir, free_images_dir)\n",
    "                               )\n",
    "\n",
    "\n",
    "print(\"Image Names: {}, Created {}, Conversion Rate {:.2f}\".format(inp, op,\n",
    "                                                                   op/inp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load defect image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed defect images (256x256)\n",
    "blowhole_img_names, blowhole_img_gt_names = get_img_and_gt_names(new_dataset_dir, blowhole_images_dir)\n",
    "break_img_names, break_img_gt_names = get_img_and_gt_names(new_dataset_dir, break_images_dir)\n",
    "crack_img_names, crack_img_gt_names = get_img_and_gt_names(new_dataset_dir, crack_images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load free image names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "free_img_names = glob.glob(os.path.join(new_dataset_dir, free_images_dir, \"*.jpg\"))\n",
    "\n",
    "train_img_names, test_img_names = train_test_split(free_img_names, train_size=2560)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final image count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free Training Images: 2560\n",
      "Free Test Images: 400\n",
      "Blowhole Test Images: 429\n",
      "Break Test Images: 283\n",
      "Crack Test Images: 214\n"
     ]
    }
   ],
   "source": [
    "print(\"Free Training Images: {}\\nFree Test Images: {}\\nBlowhole Test Images: {}\\\n",
    "\\nBreak Test Images: {}\\nCrack Test Images: {}\".format(\n",
    "                                                                                          len(train_img_names),\n",
    "                                                                                          len(test_img_names),\n",
    "                                                                                          len(blowhole_img_names),\n",
    "                                                                                          len(break_img_names),\n",
    "                                                                                          len(crack_img_names)\n",
    "                                                                                         ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_generator(training_image_names, batch_size):\n",
    "    \n",
    "    while(True):\n",
    "        training_image_names = shuffle(training_image_names)\n",
    "\n",
    "        for offset in range(0, len(training_image_names), batch_size):\n",
    "            image_set_names = training_image_names[offset:batch_size+offset]\n",
    "            training_images = load_and_normalize(image_set_names)\n",
    "\n",
    "            yield training_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_generator(testing_image_names, testing_gt_image_names, test_batch_size):\n",
    "    \n",
    "    while(True):\n",
    "        testing_image_names, testing_gt_image_names = shuffle(testing_image_names, testing_gt_image_names)\n",
    "        \n",
    "        for offset in range(0, len(testing_image_names), test_batch_size):\n",
    "            image_set_names = testing_image_names[offset:test_batch_size+offset]\n",
    "            image_gt_set_names = testing_gt_image_names[offset:test_batch_size+offset]\n",
    "            \n",
    "            testing_images = load_and_normalize(image_set_names)\n",
    "            testing_gt_images = load_and_normalize(image_gt_set_names)\n",
    "            \n",
    "            yield testing_images, testing_gt_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_dir(checkpoint_dir):\n",
    "    r = glob.glob(os.path.join(checkpoint_dir, \"logs*\"))\n",
    "    log_dir_name = os.path.join(checkpoint_dir, \"logs{}\".format(str(np.max[len(r)-1, 0])))\n",
    "                  \n",
    "    return log_dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainx_names, testx_names, \n",
    "          blowhole_x_names, blowhole_y_names, \n",
    "          crack_x_names, crack_y_names, \n",
    "          break_x_names, break_y_names\n",
    "         ):\n",
    "    \n",
    "    penultimate_layer_units = 1024\n",
    "    latent_dimensions = 200\n",
    "    batch_size = 16\n",
    "    # FREQ_PRINT = 80\n",
    "    learning_rate = 0.00005\n",
    "    nb_epochs = 500\n",
    "    freq_epoch_test = 5\n",
    "    num_test_images = 32\n",
    "    \n",
    "    # Image input placeholder\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # Ground truth input placeholder\n",
    "    gt = tf.placeholder(dtype=tf.float32, shape=(None,256,256,3))\n",
    "    \n",
    "    # mean and variance of the free image scores\n",
    "    mean_inp = tf.placeholder(dtype=tf.float32)\n",
    "    var_inp = tf.placeholder(dtype=tf.float32)\n",
    "    \n",
    "    # Training mode placeholder\n",
    "    training_mode = tf.placeholder(dtype=tf.bool)\n",
    "    \n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_real_image = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=False, training_mode=True)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        z = tf.random_normal([batch_size, latent_dimensions])\n",
    "        generated_image = get_generator_model(z, reuse=False, training_mode=True)\n",
    "        regenerated_real_image = get_generator_model(encoding_real_image, reuse=True, training_mode=False)\n",
    "    \n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake, dis_fake_penultimate_layer = get_discriminator_model(generated_image, z, reuse=False, \n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "        discriminator_real, dis_real_penultimate_layer = get_discriminator_model(x, encoding_real_image, reuse=True,\n",
    "                                                     training_mode=True, penultimate_layer_units=penultimate_layer_units)\n",
    "    \n",
    "    # Prepare labels for the loss functions\n",
    "    with tf.variable_scope('labels'):\n",
    "        \n",
    "        # Step 1\n",
    "        # Set swapped labels\n",
    "        labels_dis_enc = tf.zeros_like(discriminator_real)\n",
    "        labels_dis_gen = tf.ones_like(discriminator_fake)\n",
    "        labels_gen = tf.zeros_like(discriminator_fake)\n",
    "        labels_enc = tf.ones_like(discriminator_real)\n",
    "        \n",
    "        # Step 2\n",
    "        # Create soft labels for the discriminator\n",
    "        random_soft = tf.random_uniform(shape=(tf.shape(labels_dis_enc)), minval=0.0, maxval=0.1)\n",
    "        soft_labels_dis_enc = tf.add(labels_dis_enc, random_soft)\n",
    "        soft_labels_dis_gen = tf.subtract(labels_dis_gen, random_soft)\n",
    "\n",
    "        # Step 3\n",
    "        # With a low chance, assign noisy (swapped) labels\n",
    "        random_flip = tf.ones_like(labels_dis_enc) * tf.random_uniform(shape=(1,), minval=0, maxval=1)\n",
    "        mask = random_flip >= 0.05\n",
    "        labels_dis_enc = tf.where(mask, soft_labels_dis_enc, soft_labels_dis_gen)\n",
    "        labels_dis_gen = tf.where(mask, soft_labels_dis_gen, soft_labels_dis_enc)\n",
    "    \n",
    "    # Loss Functions\n",
    "    with tf.variable_scope('loss_functions'):\n",
    "        loss_dis_enc = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_dis_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "        loss_dis_gen = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_dis_gen,\n",
    "                                                                              logits=discriminator_fake))\n",
    "        loss_discriminator = loss_dis_gen + loss_dis_enc\n",
    "        # generator\n",
    "        loss_generator = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_gen,\n",
    "                                                                                logits=discriminator_fake))\n",
    "        # encoder\n",
    "        loss_encoder = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=labels_enc,\n",
    "                                                                              logits=discriminator_real))\n",
    "    with tf.name_scope('optimizers'):\n",
    "        # control op dependencies for batch norm and trainable variables\n",
    "        tvars = tf.trainable_variables()\n",
    "        dvars = [var for var in tvars if 'discriminator_model' in var.name]\n",
    "        gvars = [var for var in tvars if 'generator_model' in var.name]\n",
    "        evars = [var for var in tvars if 'encoder_model' in var.name]\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        update_ops_gen = [x for x in update_ops if ('generator_model' in x.name)]\n",
    "        update_ops_enc = [x for x in update_ops if ('encoder_model' in x.name)]\n",
    "        update_ops_dis = [x for x in update_ops if ('discriminator_model' in x.name)]\n",
    "\n",
    "        optimizer_dis = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='dis_optimizer')\n",
    "        optimizer_gen = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='gen_optimizer')\n",
    "        optimizer_enc = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5, name='enc_optimizer')\n",
    "\n",
    "        with tf.control_dependencies(update_ops_gen):\n",
    "            gen_op = optimizer_gen.minimize(loss_generator, var_list=gvars)\n",
    "        with tf.control_dependencies(update_ops_enc):\n",
    "            enc_op = optimizer_enc.minimize(loss_encoder, var_list=evars, global_step=tf.train.get_or_create_global_step())\n",
    "        with tf.control_dependencies(update_ops_dis):\n",
    "            dis_op = optimizer_dis.minimize(loss_discriminator, var_list=dvars)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        with tf.name_scope('discriminator'):\n",
    "            tf.summary.scalar('loss_total', loss_discriminator, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_enc', loss_dis_enc, ['dis'])\n",
    "            tf.summary.scalar('loss_dis_gen', loss_dis_gen, ['dis'])\n",
    "\n",
    "        with tf.name_scope('generator'):\n",
    "            tf.summary.scalar('loss_generator', loss_generator, ['gen'])\n",
    "            tf.summary.scalar('loss_encoder', loss_encoder, ['gen'])\n",
    "\n",
    "    with tf.name_scope('train_img_regen'):\n",
    "        for p in range(4):\n",
    "            tf.summary.image('img_{}_regen'.format(p+1), regenerated_real_image[p:p+1,:,:,:], 1, ['image'])\n",
    "            tf.summary.image('img_{}_input'.format(p+1), x[p:p+1,:,:,:], 1, ['image'])\n",
    "\n",
    "    sum_op_dis = tf.summary.merge_all('dis')\n",
    "    sum_op_gen = tf.summary.merge_all('gen')\n",
    "    sum_op_im = tf.summary.merge_all('image')\n",
    "\n",
    "        \n",
    "    '''\n",
    "    ----------------------------------------TRAINING OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "        \n",
    "    # TESTING GRAPH\n",
    "\n",
    "\n",
    "    # Encoder\n",
    "    with tf.variable_scope('encoder_model'):\n",
    "        encoding_test = get_encoder_model(x, latent_dimensions=latent_dimensions,\n",
    "                                             reuse=True, training_mode=False)\n",
    "    # Generator\n",
    "    with tf.variable_scope('generator_model'):\n",
    "        regenerated_image_test = get_generator_model(encoding_test, reuse=True, training_mode=False)\n",
    "\n",
    "    # Discriminator\n",
    "    with tf.variable_scope('discriminator_model'):\n",
    "        discriminator_fake_test, dis_fake_penultimate_layer_test = get_discriminator_model(regenerated_image_test, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True, \n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "        discriminator_real_test, dis_real_penultimate_layer_test = get_discriminator_model(x, \n",
    "                                                                                      encoding_test, \n",
    "                                                                                      reuse=True,\n",
    "                                                                                      training_mode=True, \n",
    "                                                                                      penultimate_layer_units=penultimate_layer_units\n",
    "                                                                                     )\n",
    "\n",
    "    with tf.name_scope('testing'):\n",
    "        with tf.variable_scope('reconstruction_loss'):\n",
    "            delta = x - regenerated_image_test\n",
    "            delta_flat = tf.layers.flatten(delta)\n",
    "            gen_score = tf.norm(delta_flat, ord='euclidean', axis=1,\n",
    "                              keep_dims=False, name='epsilon')\n",
    "\n",
    "        with tf.variable_scope('discriminator_loss'):\n",
    "            fm = dis_real_penultimate_layer_test - dis_fake_penultimate_layer_test\n",
    "            fm = tf.contrib.layers.flatten(fm)\n",
    "            dis_score = tf.norm(fm, ord='euclidean', axis=1,\n",
    "                             keep_dims=False, name='d_loss')\n",
    "            dis_score = tf.squeeze(dis_score)\n",
    "\n",
    "            \n",
    "        weight1, weight2, weight3, weight4, weight5 = 0.1, 0.2, 0.3, 0.4, 0.5 \n",
    "        \n",
    "        with tf.variable_scope('score'):\n",
    "            mean_score1 = tf.reduce_mean((1 - weight1) * gen_score + weight1 * dis_score)\n",
    "            mean_score2 = tf.reduce_mean((1 - weight2) * gen_score + weight2 * dis_score)\n",
    "            mean_score3 = tf.reduce_mean((1 - weight3) * gen_score + weight3 * dis_score)\n",
    "            mean_score4 = tf.reduce_mean((1 - weight4) * gen_score + weight4 * dis_score)\n",
    "            mean_score5 = tf.reduce_mean((1 - weight5) * gen_score + weight5 * dis_score)\n",
    "            \n",
    "\n",
    "    with tf.name_scope('test_anomaly_score'):\n",
    "        tf.summary.scalar(\"mean_score_w=0.1\", mean_score1, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.2\", mean_score2, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.3\", mean_score3, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.4\", mean_score4, ['scr'])\n",
    "        tf.summary.scalar(\"mean_score_w=0.5\", mean_score5, ['scr'])\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('accuracy'):\n",
    "        \n",
    "        # For defect accuracy calculation\n",
    "        all_test_scores = (1 - weight1) * gen_score + weight1 * dis_score\n",
    "        free_thresh_0 = mean_inp\n",
    "        free_thresh_1 = mean_inp + tf.sqrt(var_inp)\n",
    "        free_thresh_2 = mean_inp + 2* tf.sqrt(var_inp)\n",
    "        free_thresh_3 = mean_inp + 3 * tf.sqrt(var_inp)\n",
    "        \n",
    "        bool_list_0 = tf.greater_equal(all_test_scores, free_thresh_0)\n",
    "        test_acc_0 = tf.reduce_sum(tf.cast(bool_list_0, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_1 = tf.greater_equal(all_test_scores, free_thresh_1)\n",
    "        test_acc_1 = tf.reduce_sum(tf.cast(bool_list_1, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_2 = tf.greater_equal(all_test_scores, free_thresh_2)\n",
    "        test_acc_2 = tf.reduce_sum(tf.cast(bool_list_2, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        bool_list_3 = tf.greater_equal(all_test_scores, free_thresh_3)\n",
    "        test_acc_3 = tf.reduce_sum(tf.cast(bool_list_3, tf.int32))/tf.size(all_test_scores)\n",
    "        \n",
    "        # For calculating optimal anomaly score based on free image scores\n",
    "        mean, var = tf.nn.moments(all_test_scores, axes=[0])\n",
    "\n",
    "        \n",
    "    \n",
    "    with tf.name_scope('test_accuracy'):\n",
    "        \n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_0', test_acc_0, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_1', test_acc_1, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_2', test_acc_2, ['test_acc'])\n",
    "        tf.summary.scalar('threshold with w=0.1, stddev_3', test_acc_3, ['test_acc'])\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('test_img_regen'):\n",
    "        for p in range(2):\n",
    "            tf.summary.image('{}_0_input'.format(p+1), x[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_1_regen'.format(p+1), regenerated_image_test[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_2_ground_truth'.format(p+1), gt[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            tf.summary.image('{}_3_difference'.format(p+1), delta[p:p+1,:,:,:], 1, ['t_image'])\n",
    "            \n",
    "            \n",
    "    sum_op_scr = tf.summary.merge_all('scr')\n",
    "    sum_op_t_img = tf.summary.merge_all('t_image')\n",
    "    sum_op_test_acc = tf.summary.merge_all('test_acc')\n",
    "    \n",
    "    gs = tf.train.get_global_step()\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ----------------------------------------TEST OPS END HERE-----------------------------------------------\n",
    "    '''    \n",
    "    \n",
    "    \n",
    "    # TRAINING\n",
    "    \n",
    "    train_gen = training_generator(trainx_names, batch_size=batch_size)\n",
    "    test_gen = training_generator(testx_names, batch_size=batch_size)\n",
    "    \n",
    "    blowhole_gen = testing_generator(blowhole_x_names, blowhole_y_names, test_batch_size=num_test_images)\n",
    "    crack_gen = testing_generator(crack_x_names, crack_y_names, test_batch_size=num_test_images)\n",
    "    break_gen = testing_generator(break_x_names, break_y_names, test_batch_size=num_test_images)\n",
    "               \n",
    "            \n",
    "    checkpoint_dir = \"train/train01/\"\n",
    "    summary_dir = get_summary_dir(checkpoint_dir)\n",
    "    \n",
    "    free_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"free\"))\n",
    "    blowhole_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"blowhole\"))\n",
    "    crack_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"crack\"))\n",
    "    break_writer = tf.summary.FileWriter(os.path.join(summary_dir, \"break\"))\n",
    "\n",
    "\n",
    "    # step_saver =tf.train.CheckpointSaverHook(checkpoint_dir=checkpoint_dir, save_steps=1600, save_secs=None)\n",
    "\n",
    "    summary_saver = tf.train.SummarySaverHook(save_steps=8,\n",
    "                                              save_secs=None,\n",
    "                                              output_dir=summary_dir, \n",
    "                                              summary_op=[sum_op_dis, sum_op_gen, sum_op_im]\n",
    "                                             )\n",
    "    config = tf.ConfigProto(log_device_placement=True)\n",
    "    \n",
    "    mnt = tf.train.MonitoredTrainingSession(config=config, \n",
    "                                            hooks=[summary_saver], \n",
    "                                            save_checkpoint_steps=1600, \n",
    "                                            checkpoint_dir=checkpoint_dir\n",
    "                                           )\n",
    "\n",
    "\n",
    "    \n",
    "    with mnt as sess:\n",
    "        \n",
    "        \n",
    "        train_batch = 0\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "        while not mnt.should_stop() and epoch < nb_epochs:\n",
    "\n",
    "            begin = time.time()\n",
    "            nr_batches_train = int(len(trainx_names) / batch_size)\n",
    "\n",
    "            # get data using generator\n",
    "            trainx = next(train_gen)\n",
    "            \n",
    "            train_loss_dis, train_loss_gen, train_loss_enc = [0, 0, 0]\n",
    "\n",
    "            # training\n",
    "            for t in range(nr_batches_train):\n",
    "\n",
    "                # print(\"Starting Epoch {}, Batch {}, Step {}\".format(epoch+1, t+1, step+1))     \n",
    "                ran_from = t * batch_size\n",
    "                ran_to = (t + 1) * batch_size\n",
    "\n",
    "                # train discriminator\n",
    "                feed_dict = {x: trainx,\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "\n",
    "                _, ld, sm = sess.run([dis_op,\n",
    "                                      loss_discriminator,\n",
    "                                      sum_op_dis],\n",
    "                                     feed_dict=feed_dict)\n",
    "                train_loss_dis += ld\n",
    "\n",
    "                # train generator and encoder\n",
    "                feed_dict = {x: trainx,\n",
    "                             training_mode: True,\n",
    "                             }\n",
    "                _,_, le, lg, sm = sess.run([gen_op,\n",
    "                                            enc_op,\n",
    "                                            loss_encoder,\n",
    "                                            loss_generator,\n",
    "                                            sum_op_gen],\n",
    "                                           feed_dict=feed_dict)\n",
    "                train_loss_gen += lg\n",
    "                train_loss_enc += le\n",
    "\n",
    "\n",
    "                train_batch += 1\n",
    "                step+=1\n",
    "\n",
    "            train_loss_gen /= nr_batches_train\n",
    "            train_loss_enc /= nr_batches_train\n",
    "            train_loss_dis /= nr_batches_train\n",
    "\n",
    "            print(\"Epoch %d | time = %ds | loss gen = %.4f | loss enc = %.4f | loss dis = %.4f \"\n",
    "                  % (epoch+1, time.time() - begin, train_loss_gen, train_loss_enc, train_loss_dis))\n",
    "            \n",
    "            # Inspect reconstruction\n",
    "            if (epoch+1) % freq_epoch_test == 0:  \n",
    "                    ran_from = 0\n",
    "                    ran_to =  4\n",
    "                    sm = sess.run(sum_op_im, feed_dict={x: trainx[ran_from:ran_to],training_mode: False})\n",
    "            \n",
    "            # Test\n",
    "            \n",
    "            if (epoch+1) % freq_epoch_test == 0:\n",
    "                print(\"Evaluating\")\n",
    "                \n",
    "                # Shuffle\n",
    "                testx = next(test_gen)\n",
    "\n",
    "                blowhole_x, blowhole_y = next(blowhole_gen)\n",
    "                break_x, break_y = next(break_gen)\n",
    "                crack_x, crack_y = next(crack_gen)\n",
    "                \n",
    "\n",
    "                # Free Test\n",
    "                free_score_summary, free_t_img_summary, current_step, mean_score, var_score = \\\n",
    "                                                                            sess.run([sum_op_scr, sum_op_t_img, gs,\n",
    "                                                                                     mean, var], \n",
    "                                                                            feed_dict={x: testx,\n",
    "                                                                                       gt: np.zeros_like(testx),\n",
    "                                                                                       mean_inp: 0,\n",
    "                                                                                       var_inp: 0,\n",
    "                                                                                       training_mode: False})\n",
    "                free_writer.add_summary(free_score_summary, current_step)\n",
    "                free_writer.add_summary(free_t_img_summary, current_step)\n",
    "                free_writer.flush()\n",
    "                \n",
    "\n",
    "                # Blowhole\n",
    "                blowhole_score_summary, blowhole_t_img_summary, blowhole_acc_summary = \\\n",
    "                                                                          sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                          feed_dict={x: blowhole_x,\n",
    "                                                                                     gt: blowhole_y,\n",
    "                                                                                     mean_inp: mean_score,\n",
    "                                                                                     var_inp: var_score,\n",
    "                                                                                     training_mode: False})\n",
    "                blowhole_writer.add_summary(blowhole_score_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_t_img_summary, current_step)\n",
    "                blowhole_writer.add_summary(blowhole_acc_summary, current_step)\n",
    "                blowhole_writer.flush()\n",
    "                \n",
    "                # Crack\n",
    "                crack_score_summary, crack_t_img_summary, crack_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: crack_x, \n",
    "                                                                               gt: crack_y,\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                crack_writer.add_summary(crack_score_summary, current_step)\n",
    "                crack_writer.add_summary(crack_t_img_summary, current_step)\n",
    "                crack_writer.add_summary(crack_acc_summary, current_step)\n",
    "                crack_writer.flush()\n",
    "                    \n",
    "                # Break\n",
    "                break_score_summary, break_t_img_summary, break_acc_summary = sess.run([sum_op_scr, sum_op_t_img, sum_op_test_acc], \n",
    "                                                                    feed_dict={x: break_x,\n",
    "                                                                               gt: break_y,\n",
    "                                                                               mean_inp: mean_score,\n",
    "                                                                               var_inp: var_score,\n",
    "                                                                               training_mode: False})\n",
    "                break_writer.add_summary(break_score_summary, current_step)\n",
    "                break_writer.add_summary(break_t_img_summary, current_step)\n",
    "                break_writer.add_summary(break_acc_summary, current_step)\n",
    "                break_writer.flush()\n",
    "                \n",
    "            \n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (16, 200)\n",
      "Output shape of layer_01 is (16, 8, 8, 1024)\n",
      "Output shape of layer_02 is (16, 16, 16, 512)\n",
      "Output shape of layer_03 is (16, 32, 32, 256)\n",
      "Output shape of layer_04 is (16, 64, 64, 128)\n",
      "Output shape of layer_05 is (16, 128, 128, 64)\n",
      "Output shape of layer_06 is (16, 256, 256, 3)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (16, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (16, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (16, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (16, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (16, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (16, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (16, 200)\n",
      "Output shape of z_layer_01 is (16, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (16, 66560)\n",
      "Output shape of xz_layer_01 is (16, 1024)\n",
      "Output shape of xz_layer_02 is (16, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Encoder: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of layer_01 is (?, 128, 128, 16)\n",
      "Output shape of layer_02 is (?, 64, 64, 32)\n",
      "Output shape of layer_03 is (?, 32, 32, 64)\n",
      "Output shape of layer_04 is (?, 16, 16, 128)\n",
      "Output shape of layer_04 is (?, 200)\n",
      "\n",
      "Generator:\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of layer_01 is (?, 8, 8, 1024)\n",
      "Output shape of layer_02 is (?, 16, 16, 512)\n",
      "Output shape of layer_03 is (?, 32, 32, 256)\n",
      "Output shape of layer_04 is (?, 64, 64, 128)\n",
      "Output shape of layer_05 is (?, 128, 128, 64)\n",
      "Output shape of layer_06 is (?, 256, 256, 3)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "\n",
      "Discriminator: \n",
      "\n",
      "Input shape of x is (?, 256, 256, 3)\n",
      "Output shape of x_layer_01 is (?, 128, 128, 64)\n",
      "Output shape of x_layer_02 is (?, 64, 64, 128)\n",
      "Output shape of x_layer_03 is (?, 32, 32, 256)\n",
      "Output shape of x_layer_04 is (?, 16, 16, 512)\n",
      "Output shape of x_layer_05 is (?, 8, 8, 1024)\n",
      "\n",
      "Input shape of z is (?, 200)\n",
      "Output shape of z_layer_01 is (?, 1024)\n",
      "\n",
      "Output shape of [x,z] concat is (?, 66560)\n",
      "Output shape of xz_layer_01 is (?, 1024)\n",
      "Output shape of xz_layer_02 is (?, 1)\n",
      "WARNING:tensorflow:From <ipython-input-10-26e70a780141>:161: calling norm (from tensorflow.python.ops.linalg_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "INFO:tensorflow:Summary name mean_score_w=0.1 is illegal; using mean_score_w_0.1 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.2 is illegal; using mean_score_w_0.2 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.3 is illegal; using mean_score_w_0.3 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.4 is illegal; using mean_score_w_0.4 instead.\n",
      "INFO:tensorflow:Summary name mean_score_w=0.5 is illegal; using mean_score_w_0.5 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_0 is illegal; using threshold_with_w_0.1__stddev_0 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_1 is illegal; using threshold_with_w_0.1__stddev_1 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_2 is illegal; using threshold_with_w_0.1__stddev_2 instead.\n",
      "INFO:tensorflow:Summary name threshold with w=0.1, stddev_3 is illegal; using threshold_with_w_0.1__stddev_3 instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into train/train01/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    train(\n",
    "          train_img_names, test_img_names, \n",
    "          blowhole_img_names, blowhole_img_gt_names, \n",
    "          crack_img_names, crack_img_gt_names,\n",
    "          break_img_names, break_img_gt_names\n",
    "         )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
